{"cells":[{"cell_type":"markdown","id":"886ecf79","metadata":{"id":"886ecf79"},"source":["# **Transcription with Conversation-Level Sentiment Annotations**\n","Below, we use the Hume and GPT APIs to generate conversation-level sentiment annotations for a Zoom conversation.\n","\n","> We design a two-part pipeline to visualize Zoom meetings with conversation-level sentiment annotations. We first introduce novel metrics to capture conversation-level sentiments along three axes: comprehension, consensus, and cordiality. To obtain these metrics, we first identify each speaker's individual expressed sentiments during each of their responses. To determine speaker sentiment, we segment Zoom recordings by speaker and feed the video data, audio file (including information on voice prosity), and transcript (text content) of each segment to an off-the-shelf model that outputs a quantitative measure of the extent to which the speaker expresses 48 emotions. Afterward, for each segment, we combine the speaker's top 5 emotions with weights, uniformly sampled facial expressions, and spoken words in an instruction-tuned prompt to a multimodal large language model in order to determine conversation-level metrics.\n","\n"]},{"cell_type":"markdown","id":"VsRSpa3IbBfV","metadata":{"id":"VsRSpa3IbBfV"},"source":["# Initialization"]},{"cell_type":"code","execution_count":1,"id":"_y0U55mIXpIb","metadata":{"collapsed":true,"id":"_y0U55mIXpIb","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715014712887,"user_tz":240,"elapsed":76613,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}},"outputId":"5237bdd7-5376-41bd-836b-5530a493ee13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hume\n","  Downloading hume-0.5.1-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx[http2]<0.28.0,>=0.27.0 (from hume)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.6.4 in /usr/local/lib/python3.10/dist-packages (from hume) (2.7.1)\n","Collecting pydub<0.26.0,>=0.25.1 (from hume)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from hume) (4.11.0)\n","Collecting websockets<13.0,>=12.0 (from hume)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume) (2024.2.2)\n","Collecting httpcore==1.* (from httpx[http2]<0.28.0,>=0.27.0->hume)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume) (1.3.1)\n","Collecting h2<5,>=3 (from httpx[http2]<0.28.0,>=0.27.0->hume)\n","  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx[http2]<0.28.0,>=0.27.0->hume)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.4->hume) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.4->hume) (2.18.2)\n","Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]<0.28.0,>=0.27.0->hume)\n","  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n","Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]<0.28.0,>=0.27.0->hume)\n","  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]<0.28.0,>=0.27.0->hume) (1.2.1)\n","Installing collected packages: pydub, websockets, hyperframe, hpack, h11, httpcore, h2, httpx, hume\n","Successfully installed h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.0 hume-0.5.1 hyperframe-6.0.1 pydub-0.25.1 websockets-12.0\n","Requirement already satisfied: hume[stream] in /usr/local/lib/python3.10/dist-packages (0.5.1)\n","Requirement already satisfied: httpx[http2]<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from hume[stream]) (0.27.0)\n","Requirement already satisfied: pydantic<3.0.0,>=2.6.4 in /usr/local/lib/python3.10/dist-packages (from hume[stream]) (2.7.1)\n","Requirement already satisfied: pydub<0.26.0,>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from hume[stream]) (0.25.1)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from hume[stream]) (4.11.0)\n","Requirement already satisfied: websockets<13.0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from hume[stream]) (12.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (1.0.5)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (1.3.1)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (4.1.0)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.4->hume[stream]) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.4->hume[stream]) (2.18.2)\n","Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (6.0.1)\n","Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (4.0.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]<0.28.0,>=0.27.0->hume[stream]) (1.2.1)\n","Collecting openai\n","  Downloading openai-1.25.2-py3-none-any.whl (312 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n","Installing collected packages: openai\n","Successfully installed openai-1.25.2\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Collecting ffmpeg\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: ffmpeg\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=171a43f6414cb19953569938ff687616f058f54c8d46c86f57889f86687fa2a8\n","  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n","Successfully built ffmpeg\n","Installing collected packages: ffmpeg\n","Successfully installed ffmpeg-1.4\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Collecting webvtt-py\n","  Downloading webvtt_py-0.4.6-py3-none-any.whl (16 kB)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n","Collecting docopt (from webvtt-py)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n","Building wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=605c7749b421d1d57e21b9216a258f63570756876daba0cd88dc8ff1ed765937\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built docopt\n","Installing collected packages: docopt, webvtt-py\n","Successfully installed docopt-0.6.2 webvtt-py-0.4.6\n"]}],"source":["# Install libraries\n","!pip install hume\n","!pip install hume[stream]\n","!pip install openai\n","!pip install python-dotenv\n","!pip install pydub\n","!pip install ffmpeg\n","!pip install moviepy webvtt-py\n"]},{"cell_type":"code","execution_count":2,"id":"99e95c11","metadata":{"executionInfo":{"elapsed":4412,"status":"ok","timestamp":1715014717293,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"},"user_tz":240},"id":"99e95c11","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f143365-640f-4de3-dd72-09857eb6135b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_provider\" has conflict with protected namespace \"model_\".\n","\n","You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_resource\" has conflict with protected namespace \"model_\".\n","\n","You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n","  warnings.warn(\n"]}],"source":["import os\n","from dotenv import load_dotenv\n","from openai import OpenAI\n","import requests\n","import base64\n","from pydub import AudioSegment\n","from hume import HumeBatchClient\n","import json\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from google.colab import userdata\n","import webvtt\n","from moviepy.editor import VideoFileClip\n","from google.colab import userdata"]},{"cell_type":"code","execution_count":3,"id":"sPGk9nVAol5W","metadata":{"executionInfo":{"elapsed":1665,"status":"ok","timestamp":1715014718951,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"},"user_tz":240},"id":"sPGk9nVAol5W"},"outputs":[],"source":["os.environ['HUME_API_KEY'] = userdata.get('HUME_API_KEY')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"]},{"cell_type":"code","execution_count":4,"id":"fd4447f2","metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1715014718952,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"},"user_tz":240},"id":"fd4447f2"},"outputs":[],"source":["# Initialize Hume and OpenAI clients\n","hume_client = HumeBatchClient(os.getenv('HUME_API_KEY'))\n","openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"]},{"cell_type":"markdown","id":"7fKO10HAqmpj","metadata":{"id":"7fKO10HAqmpj"},"source":["# Dataset Preparation"]},{"cell_type":"markdown","id":"QnlMi3FBqrUW","metadata":{"id":"QnlMi3FBqrUW"},"source":["<!-- ## First prepare the recording for one person\n","\n","Below is a set of sentences using different semantic tones\n","\n","**Encouraging**: \"I'm confident that if we all pull together, we can complete the project ahead of the deadline. Let’s keep up the great work!\"\n","\n","**Urgent**: \"It’s crucial that we meet the project deadline. Every moment counts, so let’s prioritize efficiently!\"\n","\n","**Casual**: \"Just a heads-up, we gotta wrap this project up by the due date. Let's get it done and have some fun along the way!\"\n","\n","**Formal**: \"It is imperative that we adhere to the stipulated timeline for the completion of this project. Your diligent attention to the deadlines is greatly appreciated.\"\n","\n","**Optimistic**: \"With the progress we’re making, I’m sure we’ll finish the project well before the deadline. Keep up the fantastic effort!\"\n","\n","**Skeptical**: \"Considering our current pace, are we sure we can finish the project by the deadline? We might need to reassess our strategy.\"\n","\n","**Direct**: \"Finish the project by the deadline. No exceptions.\" -->\n","\n","<!-- # Get Inputs\n","load video from data folder, they are two processed Zoom recording files, with role1.mp4 and role2.mp4\n","the transcript file is transcript.vtt -->"]},{"cell_type":"markdown","id":"f7ORYCr01rE6","metadata":{"id":"f7ORYCr01rE6"},"source":["## **Step1: Segment the video by speaker**\n","\n","Zoom already segments the VTT file into clips (segments in which someone is continuously speaking). We combine adjacent clips with the same speaker and then segment the associated video file.\n"]},{"cell_type":"code","execution_count":5,"id":"91f708ac-6ff9-47b9-aba9-18a49a9b0166","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1715014718952,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"},"user_tz":240},"id":"91f708ac-6ff9-47b9-aba9-18a49a9b0166"},"outputs":[],"source":["# Define file locations\n","vtt_file = './data/zoom/clipped.vtt'\n","video_file = './data/zoom/video.mp4'\n","\n","# Define the output directory\n","output_dir = './data/zoom_clipped/'\n","\n","# Ensure the output directory exists\n","os.makedirs(output_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":6,"id":"OhpBLBRIb5UP","metadata":{"id":"OhpBLBRIb5UP","colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"status":"error","timestamp":1715014718952,"user_tz":240,"elapsed":25,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}},"outputId":"57c04dcf-a6f8-419a-a7dd-84df7b0e941d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './data/zoom/clipped.vtt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5bcc80d287d3>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Parse VTT and process clips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0muser_clips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_vtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprocess_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_clips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-5bcc80d287d3>\u001b[0m in \u001b[0;36mparse_vtt\u001b[0;34m(vtt_file)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0muser_clips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlast_speaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwebvtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webvtt/webvtt.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(cls, file)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"\"\"Reads a WebVTT captions file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebVTTParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webvtt/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"\"\"Reads the captions file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_content_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webvtt/parsers.py\u001b[0m in \u001b[0;36m_get_content_from_file\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_content_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_file_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_content_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/webvtt/parsers.py\u001b[0m in \u001b[0;36m_read_file_encoding\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_file_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfirst_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/zoom/clipped.vtt'"]}],"source":["def parse_vtt(vtt_file):\n","  '''\n","  @param vtt_file: a VTT file\n","  @returns a JSON that maps names of speakers to clips, where each clip is represented as an array of strings of form [start_timestamp, end_timestamp, text_said]\n","  '''\n","  user_clips = {}\n","  last_speaker = None\n","  for caption in webvtt.read(vtt_file):\n","      if ':' in caption.text:\n","          username, text = caption.text.split(':', 1)\n","          username = username.strip().replace(' ', '-')\n","          text = text.strip()\n","          if username not in user_clips:\n","              user_clips[username] = []\n","          if username == last_speaker:\n","            user_clips[username][-1] = (user_clips[username][-1][0], caption.end, user_clips[username][-1][2] + \" \" + text)\n","          else:\n","            user_clips[username].append((caption.start, caption.end, text))\n","\n","          last_speaker = username\n","  return user_clips\n","\n","def process_clips(video_file, user_clips):\n","    video = VideoFileClip(video_file)\n","    for user, entries in user_clips.items():\n","        for start, end, text in entries:\n","            try:\n","                # Format the filename base\n","                filename_base = f\"{user}_{start.replace(':', '-')}_{end.replace(':', '-')}\"\n","                video_filename = f\"{output_dir}{filename_base}.mp4\"\n","                text_filename = f\"{output_dir}{filename_base}.txt\"\n","                audio_filename = f\"{output_dir}{filename_base}.mp3\"\n","\n","                # Process video\n","                clip = video.subclip(start, end)\n","                clip.write_videofile(video_filename, codec=\"libx264\")\n","\n","                # Save transcript text\n","                with open(text_filename, 'w') as text_file:\n","                    text_file.write(text)\n","\n","                # Extract and save audio\n","                audio = clip.audio\n","                audio.write_audiofile(audio_filename)\n","\n","            except Exception as e:\n","                print(f\"Failed to process {filename_base}: {e}\")\n","\n","# Parse VTT and process clips\n","user_clips = parse_vtt(vtt_file)\n","process_clips(video_file, user_clips)"]},{"cell_type":"markdown","id":"xH44q1ifb7-u","metadata":{"id":"xH44q1ifb7-u"},"source":["## **Batching for Hume**\n"]},{"cell_type":"code","execution_count":null,"id":"32a7aaa1","metadata":{"id":"32a7aaa1","executionInfo":{"status":"aborted","timestamp":1715014718952,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["# Document https://humeai.github.io/hume-python-sdk/0.2.0/\n","from hume import HumeBatchClient\n","\n","# from hume.models.config import BurstConfig # for burst\n","from hume.models.config import FaceConfig # for face expression\n","from hume.models.config import LanguageConfig # for text and audio contents\n","from hume.models.config import ProsodyConfig # for audio prosody(tone)\n","\n","# Define configurations for Hume client\n","input_dir = './data/zoom_clipped/'\n","output_dir = './data/outputs/hume/'\n","\n","# Ensure the output directory exists\n","os.makedirs(output_dir, exist_ok=True)\n","\n","def process_file(filepath, config, suffix):\n","    filename = os.path.basename(filepath)\n","    file_extension = os.path.splitext(filename)[1]\n","\n","    # Submit the job\n","    job = hume_client.submit_job(None, [config], files=[filepath])\n","    print(f\"Processing {filename} with {config.__class__.__name__}...\")\n","\n","    # Await and retrieve results\n","    job.await_complete()\n","    predictions = job.get_predictions()\n","    print(predictions)\n","\n","    # Form the output filename and save the predictions\n","    output_filename = filename.replace(file_extension, suffix + '.json')\n","    output_filepath = os.path.join(output_dir, output_filename)\n","    job.download_predictions(output_filepath)\n","    print(f\"Predictions for {filename} downloaded to {output_filepath}\")\n","\n","def process_files(input_dir):\n","    # List all files in the directory\n","    files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n","\n","    # Process each file with the appropriate configurations\n","    for filepath in files:\n","        file_extension = os.path.splitext(filepath)[1]\n","\n","        # Apply different configurations based on file type\n","        if file_extension in ['.mp4', '.mov']:  # Video files\n","            # Face analysis on video\n","            process_file(filepath, FaceConfig(), '_face')\n","            # Language analysis on video, it outputs the language for the whole sentense, different from the text_language analysis\n","            # process_file(filepath, LanguageConfig(), '_video_lang')\n","            # Prosody analysis on video. it is the same as audio prosody, so we don't need it\n","            # process_file(filepath, ProsodyConfig(), '_video_prosody')\n","        elif file_extension == '.mp3':  # Audio files\n","            # Prosody analysis on audio, audio prosody outputs the tone analysis for the whole period\n","            process_file(filepath, ProsodyConfig(), '_prosody')\n","            # Language analysis on audio, it is essentially the same as text language analysis, so we don't need it\n","            # process_file(filepath, LanguageConfig(), '_audio_lang')\n","        elif file_extension == '.txt':  # Text files\n","            # Language analysis on text, it's word by word, too detail, we don't need it now\n","            process_file(filepath, LanguageConfig(), '_lang')\n","            # # Prosody analysis on text, after test, text doesn't have prosody outputs\n","            # process_file(filepath, ProsodyConfig(), '_text_prosody')\n","\n","# Process all files in the input directory\n","process_files(input_dir)"]},{"cell_type":"markdown","id":"ORVS6gLS69Yr","metadata":{"id":"ORVS6gLS69Yr"},"source":["##**Individual Sentiment Annotations**\n","\n","We call the Hume's prosody, facial expression, and language APIs on the clips we created above and isolate the top 5 emotions expressed by each speaker during each line."]},{"cell_type":"markdown","id":"OZ7KmTI1NYzI","metadata":{"id":"OZ7KmTI1NYzI"},"source":["#### **Helper Functions**"]},{"cell_type":"code","execution_count":null,"id":"hHmilSDUTHJ8","metadata":{"id":"hHmilSDUTHJ8","executionInfo":{"status":"aborted","timestamp":1715014718953,"user_tz":240,"elapsed":23,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["def predict_sentiment(modality):\n","  '''\n","  Predicts the sentiment of all clips based on the specified modality.\n","  @param modality: String suffix describing modality in files, i.e. \"face,\" \"prosody,\" or \"language\"\n","  @return Dataframe of clip timing, predicted emotions, top k emotions, confidence, and additional columns depending on the modality\n","  '''\n","  # Path to the directory containing the files\n","  directory_path = './data/outputs/hume/'\n","\n","  # List all files in the directory with the relevant suffix\n","  if modality == \"language\":\n","    suffix = \"_lang\"\n","  else:\n","    suffix = \"_\" + modality\n","  relevant_files = [f for f in os.listdir(directory_path) if suffix in f and f.endswith('.json')]\n","\n","  # List to hold all the dataframes\n","  dataframes = []\n","\n","  # Count errors\n","  errors = 0\n","\n","  # Iterate over each file and process similarly to the first file\n","  for file_name in relevant_files:\n","      file_path = os.path.join(directory_path, file_name)\n","      with open(file_path, 'r') as file:\n","          data = json.load(file)\n","          # TODO: replace with corruption checking. Removes clips with errors/file not recognized\n","          if data[0]['results']['errors']:\n","            errors += 1\n","            continue\n","          predictions = data[0]['results']['predictions'][0]['models'][modality]['grouped_predictions'][0]['predictions']\n","          df = pd.DataFrame(predictions)\n","          dataframes.append(df)\n","\n","  # Combine all dataframes into one\n","  combined_df = pd.concat(dataframes, ignore_index=True)\n","\n","  # extract top emotions\n","  df_full = top_emotions_to_df(combined_df)\n","\n","  # display\n","  print(\"Omitted clips: \", errors)\n","  display(df_full.head())\n","  return df_full\n"]},{"cell_type":"code","execution_count":null,"id":"V_F2HlOq4ewe","metadata":{"id":"V_F2HlOq4ewe","executionInfo":{"status":"aborted","timestamp":1715014718953,"user_tz":240,"elapsed":23,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["def top_emotions(emotion_list, k=5):\n","  '''\n","  @params: takes in a list of objects with properties 'name' (String, name of emotion) and 'score' (float, degree of expression)\n","  @returns a list containing the k emotions with the highest scores, in the same object format as the input\n","  '''\n","  return sorted(emotion_list, key=lambda x: x['score'], reverse=True)[:k]"]},{"cell_type":"code","execution_count":null,"id":"-IbYyHkXMU7_","metadata":{"id":"-IbYyHkXMU7_","executionInfo":{"status":"aborted","timestamp":1715014718953,"user_tz":240,"elapsed":23,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["def top_emotions_to_df(df):\n","  '''\n","  @params: takes in a dataframe with a column labeled \"emotions\"\n","  @returns: the same dataframe with an additional column labeled \"top emotions\" that has the top 5 emotions at that time stamp\n","  '''\n","  df_ranked = df['emotions'].apply(top_emotions)\n","  df_ranked = df_ranked.rename('top emotions')\n","  df_full = df.join(df_ranked)\n","  return df_full"]},{"cell_type":"code","execution_count":null,"id":"HAvu8hMZOUTr","metadata":{"id":"HAvu8hMZOUTr","executionInfo":{"status":"aborted","timestamp":1715014718953,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["def plot_emotions(df, i, title=\"Emotions Expressed\"):\n","  '''\n","  Displays a bar chart of the top k emotions experienced during interaction number i\n","  @param df: dataframe with a column 'top emotions' that has highest-scoring emotions experienced per timestamp\n","  @param i: index of the interaction to plot\n","  @returns: None\n","  '''\n","  emotions = df['top emotions'][i]\n","\n","  # Plot the emotions\n","  plt.figure()\n","  plt.bar([emotion['name'] for emotion in emotions], [emotion['score'] for emotion in emotions])\n","  plt.xlabel(\"Emotion\")\n","  plt.ylabel(\"Intensity\")\n","  plt.title(title)\n","  plt.show()"]},{"cell_type":"code","source":["def plot_multiple_emotions(df_list, i_list, titles):\n","    '''\n","    Displays bar charts for the top k emotions experienced during specified interactions, organized in subplots.\n","    @param df_list: list of dataframes, each with a column 'top emotions' that has highest-scoring emotions experienced per timestamp\n","    @param i_list: list of indices for the interactions to plot (same order as dataframes)\n","    @param titles: list of titles for each subplot\n","    @returns: None\n","    '''\n","    num_plots = len(df_list)\n","    fig, axs = plt.subplots(1, num_plots, figsize=(6 * num_plots, 5), sharey=True)\n","\n","    for ax, df, i, title in zip(axs, df_list, i_list, titles):\n","        emotions = df['top emotions'][i]\n","        ax.bar([emotion['name'] for emotion in emotions], [emotion['score'] for emotion in emotions])\n","        ax.set_xlabel(\"Emotion\")\n","        ax.set_title(title)\n","\n","    axs[0].set_ylabel(\"Intensity\")\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"mGHTYo6YjzYl","executionInfo":{"status":"aborted","timestamp":1715014718953,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"mGHTYo6YjzYl","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5JFbcaUPNO2a","metadata":{"id":"5JFbcaUPNO2a"},"source":["#### **Main**\n"]},{"cell_type":"code","execution_count":null,"id":"FcIov4nW69IC","metadata":{"executionInfo":{"elapsed":0,"status":"aborted","timestamp":1715014718954,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"},"user_tz":240},"id":"FcIov4nW69IC"},"outputs":[],"source":["prosody = predict_sentiment(\"prosody\")\n","face = predict_sentiment(\"face\")\n","language = predict_sentiment(\"language\")"]},{"cell_type":"code","source":["df_list = [prosody, face, language]\n","i_list = [0, 0, 0]\n","titles = [\n","    \"Emotions Expressed Based on Prosody\",\n","    \"Emotions in First Utterance Based on Facial Expression\",\n","    \"Emotions in First Utterance Based on Spoken Word\"\n","]\n","\n","plot_multiple_emotions(df_list, i_list, titles)"],"metadata":{"id":"XfzeGs4djeeS","executionInfo":{"status":"aborted","timestamp":1715014718955,"user_tz":240,"elapsed":24,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"XfzeGs4djeeS","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0frMVtWgEaCE","metadata":{"id":"0frMVtWgEaCE"},"source":["## zip and download the data folder"]},{"cell_type":"code","execution_count":null,"id":"2kY6kSt-Ed6v","metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1715014718955,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"},"user_tz":240},"id":"2kY6kSt-Ed6v"},"outputs":[],"source":["import os\n","import zipfile\n","from google.colab import files as colab_files  # Renaming the import to avoid conflicts\n","\n","def make_zip(output_filename, source_dir):\n","    with zipfile.ZipFile(output_filename + '.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for root, dirs, files in os.walk(source_dir):\n","            for file in files:\n","                file_path = os.path.join(root, file)\n","                # Store files relative to the source directory in the zip file\n","                zipf.write(file_path, os.path.relpath(file_path, start=source_dir))\n","    colab_files.download(output_filename + '.zip')  # Using the renamed import\n","\n","# Example usage: zip the contents of './data/' into 'data.zip'\n","make_zip('data', './data/')\n"]},{"cell_type":"markdown","id":"6AfzGYwtQL-A","metadata":{"id":"6AfzGYwtQL-A"},"source":["## **Multimodal Fusion**\n","We've identified the top 5 emotions expressed through each of the three modes: facial expression, vocal prosody, and spoken word. We try several methods to combine our analysis:\n","\n","\n","*   Passing all emotions and scores to GPT-4V\n","*   Weighting all scores by the confidence/probability value given, and selecting the top 5 emotions expressed across all three modalities.\n","\n"]},{"cell_type":"markdown","id":"px_p-_DUZkjk","metadata":{"id":"px_p-_DUZkjk"},"source":["# **ChatGPT-4V**"]},{"cell_type":"markdown","source":["##**Prompt Generation**\n","\n","Improvement for next time: clarify that GPT can and should use decimals; be stricter on the output\n","\n","The full prompt text:\n","\n","> You're an expert at understanding the social dynamics underlying conversations. In particular, you are great at monitoring three conversational metrics: comprehension, consensus, and cordiality. You produce quantitative measure of each of these metrics by averaging how each participant would rate the other on a 5-point Likert scale, with 1 = low and 5 = high. Cordiality is the extent to which each participant would rate herself as 'liking' the other person. Consensus is the extent to which each participant would rate herself as 'agreeing' with the other person. Confusion is the extent to which each participant would rate herself as 'understanding' what the other person is saying. You characterize conversations by considering each participant's facial expression, dialogue, and expressed emotions (based on vocal prosody, facial expression, and language), which will be provided to you. I will now provide you with a series of interactions. For each interaction, please reply in exactly this format (excluding quotation marks): '[CORDIALITY_SCORE],[CONSENSUS_SCORE],[CONFUSION_SCORE]' and replace the variables, which have format [VARIABLE_NAME], with your 1-5 rating."],"metadata":{"id":"ZiGfVJ2rofmA"},"id":"ZiGfVJ2rofmA"},{"cell_type":"code","execution_count":null,"id":"gHcsHR6YQ765","metadata":{"id":"gHcsHR6YQ765","executionInfo":{"status":"aborted","timestamp":1715014718955,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["def generate_prompt_df(df, modality):\n","  def generate_prompt(top_emotions, modality):\n","    '''\n","    Generates a GPT-4 prompt that incorporates the individual sentiments identified above\n","    '''\n","    prompt = f\"Judging by their {modality}, the speaker expressed the following emotions: \"\n","    for emotion in top_emotions:\n","      prompt += f\"{emotion['name']}, with an intensity of {emotion['score']}, \"\n","    prompt = prompt[:-2] + \".\" # fix punctuation\n","    return prompt\n","  prompts = df['top emotions'].apply(generate_prompt, modality=modality)\n","  prompts = prompts.rename('prompt')\n","  df_full = df.join(prompts)\n","  return df_full"]},{"cell_type":"code","source":["face_with_prompt = generate_prompt_df(face, \"face\")\n","lang_with_prompt = generate_prompt_df(language, \"language\")\n","prosody_with_prompt = generate_prompt_df(prosody, \"prosody\")"],"metadata":{"id":"0oV0hpg6lB_D","executionInfo":{"status":"aborted","timestamp":1715014718955,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"0oV0hpg6lB_D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_conversation_metrics_df(df):\n","  def generate_conversation_metric(prompt):\n","    instructions = \"You're an expert at understanding the social dynamics underlying conversations. In particular, you are great at monitoring three conversational metrics: comprehension, consensus, and cordiality. You produce quantitative measure of each of these metrics by averaging how each participant would rate the other on a 5-point Likert scale, with 1 = low and 5 = high. Cordiality is the extent to which each participant would rate herself as 'liking' the other person. Consensus is the extent to which each participant would rate herself as 'agreeing' with the other person. Confusion is the extent to which each participant would rate herself as 'understanding' what the other person is saying. You characterize conversations by considering each participant's facial expression, dialogue, and expressed emotions (based on vocal prosody, facial expression, and language), which will be provided to you. I will now provide you with a series of interactions. For each interaction, please reply in exactly this format (excluding quotation marks): '[CORDIALITY_SCORE],[CONSENSUS_SCORE],[CONFUSION_SCORE]' and replace the variables, which have format [VARIABLE_NAME], with your 1-5 rating.\"\n","    full_prompt = instructions + prompt\n","    message=[{\"role\": \"assistant\", \"content\": full_prompt}]\n","    temperature=0.2\n","    max_tokens=256\n","    frequency_penalty=0.0\n","\n","    response = openai_client.chat.completions.create(\n","        model=\"gpt-4\",\n","        messages = message,\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","        frequency_penalty=frequency_penalty\n","    )\n","    return response.choices[0].message.content\n","\n","  conv_metrics = df['prompt'].apply(generate_conversation_metric)\n","  conv_metrics = conv_metrics.rename('conv_metrics')\n","  df_full = df.join(conv_metrics)\n","  return df_full"],"metadata":{"id":"GLfdwBEeqxNX","executionInfo":{"status":"aborted","timestamp":1715014718956,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"GLfdwBEeqxNX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pros = generate_conversation_metrics_df(prosody_with_prompt)"],"metadata":{"id":"xbRX5azK4d8W","executionInfo":{"status":"aborted","timestamp":1715014718956,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"xbRX5azK4d8W","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Let's observe how the conversational metrics change over time.**\n","\n"],"metadata":{"id":"YnsgBJC-4hAQ"},"id":"YnsgBJC-4hAQ"},{"cell_type":"code","source":["conv_metrics = pros['conv_metrics'].values.tolist()"],"metadata":{"id":"6PTGzNNj2q_p","executionInfo":{"status":"aborted","timestamp":1715014718956,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"6PTGzNNj2q_p","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**We have some issues with GPT deviating from the output script, either due to a prompting mistake or because GPT switches between using ' and \" as quotation marks. Check out part of the output below.**"],"metadata":{"id":"cPYmojgs4urE"},"id":"cPYmojgs4urE"},{"cell_type":"code","source":["conv_metrics"],"metadata":{"id":"3C26No4i4ckp","executionInfo":{"status":"aborted","timestamp":1715014718956,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"3C26No4i4ckp","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Let's clean up the conversation metrics and convert them to lists so they're easier to plot.**"],"metadata":{"id":"Tk0hfndI6bXj"},"id":"Tk0hfndI6bXj"},{"cell_type":"code","source":["import re\n","\n","pattern = pattern = r'(\\d+\\.?\\d*),(\\d+\\.?\\d*),(\\d+\\.?\\d*)'\n","def parse_and_convert(strings):\n","    pattern = re.compile(r'(\\d+\\.?\\d*),(\\d+\\.?\\d*),(\\d+\\.?\\d*)')\n","    results = []\n","\n","    for string in strings:\n","        matches = pattern.search(string)\n","        if matches:\n","            numbers = [float(num) for num in matches.groups()]\n","            results.append(numbers)\n","\n","    return results"],"metadata":{"id":"AluH1doc5bus","executionInfo":{"status":"aborted","timestamp":1715014718956,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"AluH1doc5bus","execution_count":null,"outputs":[]},{"cell_type":"code","source":["parsed_data = parse_and_convert(conv_metrics)"],"metadata":{"id":"7e7zJ54i5mdn","executionInfo":{"status":"aborted","timestamp":1715014718956,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"7e7zJ54i5mdn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_points = range(len(parsed_data))\n","values1, values2, values3 = zip(*parsed_data)\n","\n","\n","fig, ax = plt.subplots()\n","\n","\n","ax.plot(time_points, values1, label='Cordiality', marker='o')\n","ax.plot(time_points, values2, label='Consensus', marker='o')\n","ax.plot(time_points, values3, label='Confusion', marker='o')\n","\n","ax.set_xlabel('Time Point')\n","ax.set_ylabel('Value')\n","ax.set_title('Values Over Time')\n","ax.legend()"],"metadata":{"id":"6vcjd6Nz6aSa","executionInfo":{"status":"aborted","timestamp":1715014718957,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"id":"6vcjd6Nz6aSa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Ideas for Improvement**\n","\n","\n","*   Few shot learning by feeding GPT some very short clips (likely to run into rate limit issues)\n","*   Instead of prompting on each speaker's response, feed in multiple exchanges so that GPT has context on the conversation\n","*   Chain of thought prompting (i.e. output what one participant would rate the other and explain why, then output what the other participant would rate the other and explain why, then give final answer in a particular (parsable) format\n","*   Make the prompt a lot shorter/cut out unnecessary words and pronouns\n","\n"],"metadata":{"id":"uIi15CbSRkmi"},"id":"uIi15CbSRkmi"},{"cell_type":"markdown","source":["# **End of Notebook**"],"metadata":{"id":"ffToOsSi7VZE"},"id":"ffToOsSi7VZE"},{"cell_type":"code","execution_count":null,"id":"mk3KUeXuVhQS","metadata":{"id":"mk3KUeXuVhQS","executionInfo":{"status":"aborted","timestamp":1715014718957,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["# convert video frames to images\n"]},{"cell_type":"code","execution_count":null,"id":"37fb7c13","metadata":{"id":"37fb7c13","executionInfo":{"status":"aborted","timestamp":1715014718957,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["# Function to encode an image to base64\n","def encode_image(image_path):\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n","\n","# Encode the image and prepare for upload\n","image_path = \"./data/david_hume.jpeg\"\n","base64_image = encode_image(image_path)\n"]},{"cell_type":"code","execution_count":null,"id":"efabbceb","metadata":{"id":"efabbceb","executionInfo":{"status":"aborted","timestamp":1715014718957,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["# Using OpenAI's GPT to understand image contents\n","headers = {\n","    \"Content-Type\": \"application/json\",\n","    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n","}\n","\n","payload = {\n","    \"model\": \"gpt-4-turbo\",\n","    \"messages\": [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n","                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n","            ],\n","        },\n","    ],\n","    \"max_tokens\": 300,\n","}\n","\n","response = requests.post(\n","    \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",")\n","\n","print(response.json())\n","\n","# limit the word count\n","# JSON mode\n","# does it maintain attention for each post\n","# rubric and scale\n","# one slider for conversation going on well\n","# judge the conversation\n","\n"]},{"cell_type":"code","execution_count":null,"id":"90867938","metadata":{"id":"90867938","executionInfo":{"status":"aborted","timestamp":1715014718957,"user_tz":240,"elapsed":22,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["# Transcribe audio using OpenAI's Whisper\n","def transcribe(audio_path):\n","    with open(audio_path, \"rb\") as audio_file:\n","        transcription = openai_client.audio.transcriptions.create(\n","            model=\"whisper-1\", file=audio_file, response_format=\"text\"\n","        )\n","        return transcription\n","\n","audio_path = \"segment_1.mp3\"\n","transcription = transcribe(audio_path)\n","print(transcription)\n"]},{"cell_type":"code","execution_count":null,"id":"76e44083","metadata":{"id":"76e44083","executionInfo":{"status":"aborted","timestamp":1715014718959,"user_tz":240,"elapsed":24,"user":{"displayName":"Selena Zhang","userId":"00301362224015756751"}}},"outputs":[],"source":["# Segment a longer audio into manageable parts\n","def segment_audio(audio_path, segment_duration_ms):\n","    song = AudioSegment.from_mp3(audio_path)\n","    segments = []\n","    for i in range(0, len(song), segment_duration_ms):\n","        segment = song[i:i+segment_duration_ms]\n","        segment_path = f\"segment_{i//segment_duration_ms}.mp3\"\n","        segment.export(segment_path, format=\"mp3\")\n","        segments.append(segment_path)\n","    return segments\n","\n","# Example usage\n","segmented_audio_paths = segment_audio(\"long_audio.mp3\", 10 * 60 * 1000)  # 10 minutes in ms\n","print(\"Segmented audio into:\", segmented_audio_paths)\n"]}],"metadata":{"colab":{"collapsed_sections":["0frMVtWgEaCE"],"provenance":[]},"kernelspec":{"display_name":"mui","language":"python","name":"mui"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}