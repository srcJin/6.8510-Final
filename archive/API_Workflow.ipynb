{
   "cells": [
    {
     "cell_type": "markdown",
     "id": "886ecf79",
     "metadata": {},
     "source": [
      "# Media File Processing with Hume and OpenAI APIs\n",
      "This notebook demonstrates how to use the Hume API for processing batches of media files, and how to use OpenAI's GPT and Whisper models for advanced tasks like image understanding and audio transcription."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "99e95c11",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Import necessary libraries\n",
      "from hume import HumeBatchClient\n",
      "from hume.models.config import FaceConfig\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "from openai import OpenAI\n",
      "import requests\n",
      "import base64\n",
      "from pydub import AudioSegment\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "2f0f9f89",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Load environment variables\n",
      "load_dotenv()\n",
      "\n",
      "HUME_API_KEY = os.getenv(\"HUME_API_KEY\")\n",
      "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "print(\"HUME_API_KEY=\", HUME_API_KEY)\n",
      "print(\"OPENAI_API_KEY=\", OPENAI_API_KEY)\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "fd4447f2",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Initialize Hume and OpenAI clients\n",
      "hume_client = HumeBatchClient(HUME_API_KEY)\n",
      "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "32a7aaa1",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Define configurations for Hume client\n",
      "filepaths = [\"faces.zip\", \"david_hume.jpeg\"]\n",
      "config = FaceConfig()\n",
      "job = hume_client.submit_job(None, [config], files=filepaths)\n",
      "print(job)\n",
      "\n",
      "print(\"Running...\")\n",
      "details = job.await_complete()\n",
      "job.download_predictions(\"predictions.json\")\n",
      "print(\"Predictions downloaded to predictions.json\")\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "37fb7c13",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Function to encode an image to base64\n",
      "def encode_image(image_path):\n",
      "    with open(image_path, \"rb\") as image_file:\n",
      "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
      "\n",
      "# Encode the image and prepare for upload\n",
      "image_path = \"david_hume.jpeg\"\n",
      "base64_image = encode_image(image_path)\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "efabbceb",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Using OpenAI's GPT to understand image contents\n",
      "headers = {\n",
      "    \"Content-Type\": \"application/json\",\n",
      "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
      "}\n",
      "\n",
      "payload = {\n",
      "    \"model\": \"gpt-4-turbo\",\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": [\n",
      "                {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n",
      "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
      "            ],\n",
      "        },\n",
      "    ],\n",
      "    \"max_tokens\": 300,\n",
      "}\n",
      "\n",
      "response = requests.post(\n",
      "    \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",
      ")\n",
      "\n",
      "print(response.json())\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "90867938",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Transcribe audio using OpenAI's Whisper\n",
      "def transcribe(audio_path):\n",
      "    with open(audio_path, \"rb\") as audio_file:\n",
      "        transcription = openai_client.audio.transcriptions.create(\n",
      "            model=\"whisper-1\", file=audio_file, response_format=\"text\"\n",
      "        )\n",
      "        return transcription\n",
      "\n",
      "audio_path = \"segment_1.mp3\"\n",
      "transcription = transcribe(audio_path)\n",
      "print(transcription)\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "id": "76e44083",
     "metadata": {},
     "outputs": [],
     "source": [
      "# Segment a longer audio into manageable parts\n",
      "def segment_audio(audio_path, segment_duration_ms):\n",
      "    song = AudioSegment.from_mp3(audio_path)\n",
      "    segments = []\n",
      "    for i in range(0, len(song), segment_duration_ms):\n",
      "        segment = song[i:i+segment_duration_ms]\n",
      "        segment_path = f\"segment_{i//segment_duration_ms}.mp3\"\n",
      "        segment.export(segment_path, format=\"mp3\")\n",
      "        segments.append(segment_path)\n",
      "    return segments\n",
      "\n",
      "# Example usage\n",
      "segmented_audio_paths = segment_audio(\"long_audio.mp3\", 10 * 60 * 1000)  # 10 minutes in ms\n",
      "print(\"Segmented audio into:\", segmented_audio_paths)\n"
     ]
    }
   ],
   "metadata": {},
   "nbformat": 4,
   "nbformat_minor": 5
  }
  