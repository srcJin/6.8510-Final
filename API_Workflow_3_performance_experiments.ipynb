{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886ecf79",
   "metadata": {
    "id": "886ecf79"
   },
   "source": [
    "# **Notebook 3: Fusion Experiments**\n",
    "In this notebook, we conduct experiments to answer two questions:\n",
    "1. How should we combine the outputs of each input modality?\n",
    "2. How should we predict basic emotions and binary sentiment from complex emotion?\n",
    "\n",
    "Read on for a more detailed explanation of both questions :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VsRSpa3IbBfV",
   "metadata": {
    "id": "VsRSpa3IbBfV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Initialization and Data Processing**\n",
    "We load a JSON of the sentence-by-sentence Hume predictions on the full MELD dataset. See the previous workbooks for how that JSON is generated and cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e95c11",
   "metadata": {
    "id": "99e95c11"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f516e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_df(directory):\n",
    "    '''\n",
    "    converts JSON output to a PD dataframe\n",
    "    each JSON contains the predicted emotions for one sentence\n",
    "    '''\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = json.load(file)\n",
    "                metadata = content['metadata']\n",
    "\n",
    "                # sentiment data\n",
    "                face_emotions = lowercase_keys(content['predicted']['face'])\n",
    "                prosody_emotions = lowercase_keys(content['predicted']['prosody'])\n",
    "                lang_emotions = lowercase_keys(content['predicted']['lang'])\n",
    "\n",
    "                data.append({\n",
    "                    'dialogue_id': metadata['dialogue_id'],\n",
    "                    'time_start': metadata['time_start'],\n",
    "                    'time_end': metadata['time_end'],\n",
    "                    'speaker': metadata['speaker'],\n",
    "                    'emotion': metadata['emotion'],\n",
    "                    'sentiment': metadata['sentiment'],\n",
    "                    'text_content': metadata['text_content'],\n",
    "                    'file_name': metadata['file_name'],\n",
    "                    'face': face_emotions,\n",
    "                    'prosody': prosody_emotions,\n",
    "                    'lang': lang_emotions\n",
    "                })\n",
    "                \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df6b370-4e71-4d2e-9b4a-9b97cc2fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_keys(dictionary):\n",
    "    '''\n",
    "    changes all (String) keys in a dictionary to be fully lower case\n",
    "    '''\n",
    "    return {key.lower(): value for key, value in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020de35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = './dataset/outputs/merged_all'\n",
    "df = load_json_to_df(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec4a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_scores(df):\n",
    "    '''\n",
    "    adds a column for each Hume/complex emotion, which contains a list of form [face_intensity, prosody_intensity, language_intensity]\n",
    "    '''\n",
    "    df_with_emotions = df.copy()\n",
    "    for emotion in all_emotions:\n",
    "        df_with_emotions[emotion] = df.apply(lambda row: [\n",
    "            row['face'].get(emotion, None),\n",
    "            row['prosody'].get(emotion, None),\n",
    "            row['lang'].get(emotion, None)\n",
    "        ], axis=1)\n",
    "            \n",
    "    return df_with_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260b52e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Exploring Our Emotion Data**\n",
    "Hume has three models that predict emotion based on different modality. One model predicts based on language (the words spoken), another based on prosody (tone of voice, pauses, and vocables), and the last on facial expression. The prosody and face models output 48 emotions. The language model outputs 53 emotions. The five additional emotions output by the language model are {'Annoyance', 'Disapproval', 'Enthusiasm', 'Gratitude', 'Sarcasm'}.\n",
    "\n",
    "Critically, the MELD dataset contains many sentences labeled with the 7 basic emotions (anger, sadness, fear, joy, surprise, disgust, and neutral). The Hume outputs are _not_ the same as the MELD dataset labels. Going forward, we will refer to the Hume outputs as \"complex emotions\" and the MELD dataset labels as \"basic emotions.\" As you'll see below, a key part of our work is reducing predictions about \"complex emotions\" to predictions about \"basic emotions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7355fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputted emotions in lang, prosody, and face models: 53 48 48\n",
      "Emotions outputted by language model but not prosody model: {'gratitude', 'disapproval', 'annoyance', 'sarcasm', 'enthusiasm'}\n",
      "Emotions outputted by prosody model but not face model: set()\n",
      "All emotions: ['admiration', 'adoration', 'aesthetic appreciation', 'amusement', 'anger', 'annoyance', 'anxiety', 'awe', 'awkwardness', 'boredom', 'calmness', 'concentration', 'confusion', 'contemplation', 'contempt', 'contentment', 'craving', 'desire', 'determination', 'disappointment', 'disapproval', 'disgust', 'distress', 'doubt', 'ecstasy', 'embarrassment', 'empathic pain', 'enthusiasm', 'entrancement', 'envy', 'excitement', 'fear', 'gratitude', 'guilt', 'horror', 'interest', 'joy', 'love', 'nostalgia', 'pain', 'pride', 'realization', 'relief', 'romance', 'sadness', 'sarcasm', 'satisfaction', 'shame', 'surprise (negative)', 'surprise (positive)', 'sympathy', 'tiredness', 'triumph']\n"
     ]
    }
   ],
   "source": [
    "lang_emotions = set(df['lang'][0].keys())\n",
    "prosody_emotions = set(df['prosody'][0].keys())\n",
    "face_emotions = set(df['face'][0].keys())\n",
    "\n",
    "print(\"Number of outputted emotions in lang, prosody, and face models:\", len(lang_emotions), len(prosody_emotions), len(face_emotions))\n",
    "print(\"Emotions outputted by language model but not prosody model:\", lang_emotions-prosody_emotions)\n",
    "print(\"Emotions outputted by prosody model but not face model:\", prosody_emotions-face_emotions)\n",
    "\n",
    "all_emotions = sorted(list(lang_emotions))\n",
    "print('All emotions:', all_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ef820-b517-478d-8d8b-1bcfb198db18",
   "metadata": {},
   "source": [
    "### **Emotion Maps**\n",
    "\n",
    "Below, we create maps from the output of Hume AI (up to 53 emotions) to the seven basic emotions in our dataset. We also map the Hume AI emotions to positive/negative sentiment. Annotations below are manually created. Interesting future exploration could involve mapping each Hume/complex emotion to a weighted sum of the 7 basic emotions, potentially through training an ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4123edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_TO_COMPLEX = {\n",
    "  'anger': ['anger', 'annoyance', 'disapproval'],\n",
    "  'fear': ['anxiety', 'doubt', 'fear', 'horror'],\n",
    "  'joy': ['admiration', 'adoration', 'amusement', 'contentment', 'desire', 'ecstasy', 'enthusiasm', 'entrancement', 'excitement', 'gratitude', 'joy', 'love', 'pride', 'relief', 'romance', 'triumph'],\n",
    "  'sadness': ['disappointment', 'distress', 'empathic pain', 'guilt', 'nostalgia', 'pain', 'sadness'],\n",
    "  'surprise': ['awe', 'confusion', 'realization', 'surprise (negative)', 'surprise (positive)'],\n",
    "  'disgust': ['contempt', 'disgust', 'envy', 'sarcasm'],\n",
    "  'neutral': ['aesthetic appreciation', 'awkwardness', 'boredom', 'calmness', 'concentration', 'contemplation', 'craving', 'determination', 'embarrassment', 'interest', 'satisfaction', 'shame', 'sympathy', 'tiredness']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99fad0fe-1e8f-4cc6-9f1f-f320a0e10730",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_TO_EMOTION = {\n",
    "  'positive': [\n",
    "    'admiration', 'adoration', 'aesthetic appreciation', 'amusement', 'awe', 'contentment', 'desire', 'ecstasy', 'enthusiasm', 'entrancement', 'excitement', 'gratitude', 'joy', 'love', 'pride', 'relief', 'romance', 'triumph'\n",
    "  ],\n",
    "  'negative': [\n",
    "    'anger', 'annoyance', 'anxiety', 'awkwardness', 'boredom', 'contempt', 'confusion', 'craving', 'disappointment', 'disapproval', 'disgust', 'distress', 'doubt', 'empathic pain', 'embarrassment', 'envy', 'fear', 'guilt', 'horror', 'nostalgia', 'pain', 'sadness', 'sarcasm', 'shame', 'surprise (negative)', 'sympathy', 'tiredness'\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7befd-85b5-44a5-b24e-1f4883e74785",
   "metadata": {},
   "source": [
    "## **Methods To Fuse Modalities**\n",
    "Hume has a facial expression model (predicts based on frame capture of facial expressions in a video), a prosody model (predicts based on signal waveform), and language model (predicts based off words spoken). Each model outputs predictions for up to 53 emotions based on the given input modality. We experiment with two methods to combine the outputs from different modalities to obtain a single number representing the intensity of each emotion.\n",
    "1. The first method is a simple sum. To get the intensity of an emotion, this function sums the intensity predicated for each modality. In other words: `awe_intensity = face_awe_intensity + prosody_awe_intensity + lang_awe_intensity`\n",
    "2. The second method is a relative sum. To get the intensity of an emotion, this function takes sums the intensity predicated for each modality, weighted by the predictive accuracy of that modality alone. In other words: `awe_intensity = face_awe_intensity * relative accuracy of face-only prediction + ... (the same for prosody and language)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42216b4e-170b-40b8-bb38-dafbbface205",
   "metadata": {},
   "source": [
    "### **Method 1: Simple Sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e262ed7a-5680-421d-bc2f-df7d964b2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_sum(df):\n",
    "    '''\n",
    "    to get the intensity of an emotion, this function sums the intensity predicated for each modality. In other words:\n",
    "    awe_intensity = face_awe_intensity + prosody_awe_intensity + lang_awe_intensity\n",
    "    '''\n",
    "    simple_sum_df = df.copy()\n",
    "    for emotion in all_emotions:\n",
    "        simple_sum_df[emotion] = df[emotion].apply(lambda x: sum([i for i in x if i is not None]))\n",
    "    return simple_sum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018bf0c-6432-4b3d-9443-d19a20121bdb",
   "metadata": {},
   "source": [
    "### **Method 2: Relative Sum**\n",
    "We calculate the extent to which each individual modality predicts the final emotion, and use the relative accuracy of each modality to weight the final sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f630fc-1ed3-4207-a16e-7c123b58d0d0",
   "metadata": {},
   "source": [
    "#### **2a: Accuracy of Individual Modalities**\n",
    "First, we test each individual modality to see how predictive it is of the final emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b17183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df, mapping, modality):\n",
    "    '''\n",
    "    predicts which basic emotion dominates by taking the mean of all complex emotions corresponding to that basic emotion, and choosing the basic emotion with the highest intensity\n",
    "    '''\n",
    "    intensities = pd.DataFrame()\n",
    "    basic_emotions = sorted(list(mapping.keys()))\n",
    "    \n",
    "    for basic_emotion in basic_emotions:\n",
    "        # this is a disgusting list comprehension that came from flattening a long loop\n",
    "        complex_emotions = pd.concat([df[modality].apply(lambda row: row.get(complex_emotion, np.nan)).rename(complex_emotion) for complex_emotion in mapping[basic_emotion]], axis=1)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            intensities[basic_emotion] = np.nanmean(complex_emotions, axis=1) # column names: basic emotions; each row is a sentence; values are the intensities of the basic emotion for that sentence\n",
    "\n",
    "    # drop all rows that have all nan values\n",
    "    intensities = intensities.dropna(how='all')\n",
    "    \n",
    "    y_pred = intensities.idxmax(axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbd63eb-ea30-4cd4-bc12-5e65c8edc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df, target, modality):\n",
    "    '''\n",
    "    reports the accuracy of the approach above compared to ground truth\n",
    "    @param df with a column `y_pred` and a column with the name `target`\n",
    "    @returns the percent of rows that have identical `y_pred` and `target` values\n",
    "    ''' \n",
    "    if target == 'emotion':\n",
    "        mapping = BASIC_TO_COMPLEX\n",
    "    elif target == 'sentiment':\n",
    "        mapping = SENTIMENT_TO_EMOTION\n",
    "    else:\n",
    "        raise Exception('Invalid target')\n",
    "        \n",
    "    y_pred = get_predictions(df, mapping, modality)\n",
    "    comparable_columns = df[target].loc[y_pred.index]\n",
    "    total_sentences = len(comparable_columns)\n",
    "    accuracy = np.sum(y_pred == comparable_columns) / total_sentences\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff9c7c05-f597-492e-95c5-b56e305523ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_weights(df, show_results=False):\n",
    "    # run on all combinations of emotions and modalities\n",
    "    targets = ['emotion', 'sentiment']\n",
    "    modalities = ['face','prosody','lang']\n",
    "    \n",
    "    results = pd.DataFrame(index=targets, columns=modalities)\n",
    "    \n",
    "    for target in targets:\n",
    "        for modality in modalities:\n",
    "            accuracy = get_accuracy(df, target, modality)\n",
    "            results.at[target, modality] = accuracy\n",
    "    results = results.astype(float)\n",
    "\n",
    "    # calculate and return relative weights\n",
    "    relative_weights = results.apply(lambda row: row / np.sum(row), axis=1)\n",
    "    \n",
    "    if show_results:\n",
    "        print(\"Accuracy by Modality\")\n",
    "        display(results)\n",
    "        print(\"Relative Modality Weights\")\n",
    "        display(relative_weights)\n",
    "    \n",
    "        # plot heatmap\n",
    "        fig, ax = plt.subplots()\n",
    "        heatmap = ax.imshow(results, cmap='viridis', interpolation='nearest')\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(modalities)))\n",
    "        ax.set_yticks(np.arange(len(targets)))\n",
    "        ax.set_xticklabels(modalities)\n",
    "        ax.set_yticklabels(targets)\n",
    "        plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
    "        plt.colorbar(heatmap)\n",
    "        \n",
    "        ax.set_title('Accuracy Scores by Target and Modality')\n",
    "        plt.xlabel('Modalities')\n",
    "        plt.ylabel('Targets')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return relative_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91148db9-e273-4005-8022-d3815e47ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Modality\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face</th>\n",
       "      <th>prosody</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>0.223697</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.357610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.399052</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.427638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               face   prosody      lang\n",
       "emotion    0.223697  0.280374  0.357610\n",
       "sentiment  0.399052  0.400000  0.427638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Modality Weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face</th>\n",
       "      <th>prosody</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>0.259605</td>\n",
       "      <td>0.325380</td>\n",
       "      <td>0.415014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.325308</td>\n",
       "      <td>0.326081</td>\n",
       "      <td>0.348611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               face   prosody      lang\n",
       "emotion    0.259605  0.325380  0.415014\n",
       "sentiment  0.325308  0.326081  0.348611"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unfused_data = get_emotion_scores(df)\n",
    "weights = calc_relative_weights(unfused_data, show_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc58fa1-903a-4873-a8e5-b9b2cf75e789",
   "metadata": {},
   "source": [
    "#### **2b: Fusion**\n",
    "And now let's put those relative weights to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d02dae-2f97-4fcd-92f0-6ef10a17f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_weights(df, target):\n",
    "    '''\n",
    "    to get the intensity of an emotion, this function takes sums the intensity predicated for each modality, weighted by the predictive accuracy of that modality alone. In other words:\n",
    "    awe_intensity = face_awe_intensity * relative accuracy of face-only prediction + ... (the same for prosody and language)\n",
    "    '''\n",
    "    weights_df = df.copy()\n",
    "    # df with rows of form [face_weight, prosody_weight, lang_weight]\n",
    "    weights = calc_relative_weights(df)\n",
    "    target_weights = list(weights.loc[target])\n",
    "    for emotion in all_emotions:\n",
    "        weights_df[emotion] = df[emotion].apply(lambda row: sum([modality * weight for modality, weight in zip(row, target_weights) if modality is not None]))\n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddfd17-e955-410f-a7d8-8ad568bf68f0",
   "metadata": {},
   "source": [
    "#### **Fused Data**\n",
    "You can see the results below for unfused data, the simple sum, and the relative sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3805800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.051815811544656754, 0.015896273776888847, 0...</td>\n",
       "      <td>[0.046310193836688995, 0.005779789295047522, 0...</td>\n",
       "      <td>[0.04528449475765228, 0.008344665169715881, 0....</td>\n",
       "      <td>[0.05977936461567879, 0.06531573086977005, 0.0...</td>\n",
       "      <td>[0.08618035167455673, 0.014928092248737812, 0....</td>\n",
       "      <td>[None, None, 0.0787773035466671]</td>\n",
       "      <td>[0.2048976570367813, 0.22192993760108948, 0.01...</td>\n",
       "      <td>[0.05436728522181511, 0.02884811908006668, 0.0...</td>\n",
       "      <td>[0.15164369344711304, 0.03940080106258392, 0.0...</td>\n",
       "      <td>[0.3025617003440857, 0.0033794636838138103, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.03606966510415077, 0.003395841922610998, 0....</td>\n",
       "      <td>[0.5214796662330627, 0.023176860064268112, 0.0...</td>\n",
       "      <td>[None, None, 0.045924624986946584]</td>\n",
       "      <td>[0.08109661191701889, 0.00422081770375371, 0.0...</td>\n",
       "      <td>[0.13604705035686493, 0.03896016255021095, 0.0...</td>\n",
       "      <td>[0.0438678115606308, 0.12426678091287613, 0.65...</td>\n",
       "      <td>[0.021023310720920563, 0.05467357486486435, 0....</td>\n",
       "      <td>[0.07115799933671951, 0.014430238865315914, 0....</td>\n",
       "      <td>[0.34018707275390625, 0.013311447575688362, 0....</td>\n",
       "      <td>[0.017957456409931183, 0.0021609694231301546, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0724908784031868, 0.003055910812690854, 0.0...</td>\n",
       "      <td>[0.0441509485244751, 0.0016689085168763995, 0....</td>\n",
       "      <td>[0.04624519497156143, 0.004924003966152668, 0....</td>\n",
       "      <td>[0.11401460319757462, 0.029382823035120964, 0....</td>\n",
       "      <td>[0.11253754794597626, 0.08178599923849106, 0.0...</td>\n",
       "      <td>[None, None, 0.09205344319343567]</td>\n",
       "      <td>[0.1962161660194397, 0.03035125695168972, 0.02...</td>\n",
       "      <td>[0.21560074388980865, 0.030698880553245544, 0....</td>\n",
       "      <td>[0.2403903752565384, 0.036647979170084, 0.0196...</td>\n",
       "      <td>[0.2060079425573349, 0.013258460909128189, 0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.025960002094507217, 0.0028873037081211805, ...</td>\n",
       "      <td>[0.19044746458530426, 0.012958797626197338, 0....</td>\n",
       "      <td>[None, None, 0.028950467705726624]</td>\n",
       "      <td>[0.06389091908931732, 0.004681485705077648, 0....</td>\n",
       "      <td>[0.06486066430807114, 0.008367716334760189, 0....</td>\n",
       "      <td>[0.23657678067684174, 0.3340807259082794, 0.23...</td>\n",
       "      <td>[0.13827574253082275, 0.06782031059265137, 0.1...</td>\n",
       "      <td>[0.05339166149497032, 0.005844631232321262, 0....</td>\n",
       "      <td>[0.1670031100511551, 0.005232616793364286, 0.0...</td>\n",
       "      <td>[0.029141930863261223, 0.006851397454738617, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.11921197921037674, 0, 0.01532479259185493]</td>\n",
       "      <td>[0.13816586136817932, 0, 0.01868689541394512]</td>\n",
       "      <td>[0.06885050237178802, 0, 0.016114022272328537]</td>\n",
       "      <td>[0.21876591444015503, 0, 0.026555464913447697]</td>\n",
       "      <td>[0.05607566609978676, 0, 0.008554211778876683]</td>\n",
       "      <td>[None, 0, 0.18107843461136022]</td>\n",
       "      <td>[0.17899048328399658, 0, 0.007928416909029087]</td>\n",
       "      <td>[0.07669304311275482, 0, 0.014362408236290017]</td>\n",
       "      <td>[0.17239488661289215, 0, 0.08554266517361005]</td>\n",
       "      <td>[0.2141619473695755, 0, 0.257558507223924]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.10045278072357178, 0, 0.016710910635689896]</td>\n",
       "      <td>[0.2172575294971466, 0, 0.025304595318933327]</td>\n",
       "      <td>[None, 0, 0.08060755083958308]</td>\n",
       "      <td>[0.29512691497802734, 0, 0.05323744503160318]</td>\n",
       "      <td>[0.11326480656862259, 0, 0.009244329140832027]</td>\n",
       "      <td>[0.037892572581768036, 0, 0.037050863107045494]</td>\n",
       "      <td>[0.032265614718198776, 0, 0.0729232303177317]</td>\n",
       "      <td>[0.05577797442674637, 0, 0.01054172085908552]</td>\n",
       "      <td>[0.35597968101501465, 0, 0.08751298813149333]</td>\n",
       "      <td>[0.05453604459762573, 0, 0.011231241204465428]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.11233010143041611, 0.0022868551313877106, 0...</td>\n",
       "      <td>[0.0696071982383728, 0.00555694755166769, 0.01...</td>\n",
       "      <td>[0.06688571721315384, 0.004095192067325115, 0....</td>\n",
       "      <td>[0.1629549264907837, 0.14536771178245544, 0.02...</td>\n",
       "      <td>[0.08566178381443024, 0.06063935160636902, 0.1...</td>\n",
       "      <td>[None, None, 0.3909915164113045]</td>\n",
       "      <td>[0.15674979984760284, 0.08858616650104523, 0.0...</td>\n",
       "      <td>[0.2349795550107956, 0.0018904487369582057, 0....</td>\n",
       "      <td>[0.25021985173225403, 0.6390978097915649, 0.04...</td>\n",
       "      <td>[0.2568792998790741, 0.014520961791276932, 0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.038216110318899155, 0.005263861268758774, 0...</td>\n",
       "      <td>[0.12238343805074692, 0.05931299924850464, 0.0...</td>\n",
       "      <td>[None, None, 0.03817640943452716]</td>\n",
       "      <td>[0.09828534722328186, 0.009946794249117374, 0....</td>\n",
       "      <td>[0.051911190152168274, 0.2808518707752228, 0.0...</td>\n",
       "      <td>[0.17751213908195496, 0.08215691894292831, 0.2...</td>\n",
       "      <td>[0.14006930589675903, 0.024958673864603043, 0....</td>\n",
       "      <td>[0.05296207591891289, 0.13199752569198608, 0.0...</td>\n",
       "      <td>[0.15917706489562988, 0.01297526340931654, 0.0...</td>\n",
       "      <td>[0.03596600890159607, 0.0024232380092144012, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.13762447237968445, 0.011282742023468018, 0....</td>\n",
       "      <td>[0.1846502423286438, 0.013612054288387299, 0.0...</td>\n",
       "      <td>[0.09273673593997955, 0.008143377490341663, 0....</td>\n",
       "      <td>[0.2993708550930023, 0.029747437685728073, 0.0...</td>\n",
       "      <td>[0.024045540019869804, 0.04573961719870567, 0....</td>\n",
       "      <td>[None, None, 0.17476380243897438]</td>\n",
       "      <td>[0.07571069151163101, 0.0635836198925972, 0.00...</td>\n",
       "      <td>[0.06732875853776932, 0.021565068513154984, 0....</td>\n",
       "      <td>[0.14015096426010132, 0.04658352956175804, 0.0...</td>\n",
       "      <td>[0.23363099992275238, 0.005424466449767351, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.12284506857395172, 0.011654037982225418, 0....</td>\n",
       "      <td>[0.12852677702903748, 0.1803823858499527, 0.00...</td>\n",
       "      <td>[None, None, 0.060818194411695004]</td>\n",
       "      <td>[0.5955402851104736, 0.01081307977437973, 0.05...</td>\n",
       "      <td>[0.05567222088575363, 0.04099632054567337, 0.0...</td>\n",
       "      <td>[0.0194998811930418, 0.11610786616802216, 0.01...</td>\n",
       "      <td>[0.022709805518388748, 0.035930514335632324, 0...</td>\n",
       "      <td>[0.05819540470838547, 0.05966867133975029, 0.0...</td>\n",
       "      <td>[0.35029497742652893, 0.007299954537302256, 0....</td>\n",
       "      <td>[0.06962665170431137, 0.005042276810854673, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          admiration  \\\n",
       "0  [0.051815811544656754, 0.015896273776888847, 0...   \n",
       "1  [0.0724908784031868, 0.003055910812690854, 0.0...   \n",
       "2      [0.11921197921037674, 0, 0.01532479259185493]   \n",
       "3  [0.11233010143041611, 0.0022868551313877106, 0...   \n",
       "4  [0.13762447237968445, 0.011282742023468018, 0....   \n",
       "\n",
       "                                           adoration  \\\n",
       "0  [0.046310193836688995, 0.005779789295047522, 0...   \n",
       "1  [0.0441509485244751, 0.0016689085168763995, 0....   \n",
       "2      [0.13816586136817932, 0, 0.01868689541394512]   \n",
       "3  [0.0696071982383728, 0.00555694755166769, 0.01...   \n",
       "4  [0.1846502423286438, 0.013612054288387299, 0.0...   \n",
       "\n",
       "                              aesthetic appreciation  \\\n",
       "0  [0.04528449475765228, 0.008344665169715881, 0....   \n",
       "1  [0.04624519497156143, 0.004924003966152668, 0....   \n",
       "2     [0.06885050237178802, 0, 0.016114022272328537]   \n",
       "3  [0.06688571721315384, 0.004095192067325115, 0....   \n",
       "4  [0.09273673593997955, 0.008143377490341663, 0....   \n",
       "\n",
       "                                           amusement  \\\n",
       "0  [0.05977936461567879, 0.06531573086977005, 0.0...   \n",
       "1  [0.11401460319757462, 0.029382823035120964, 0....   \n",
       "2     [0.21876591444015503, 0, 0.026555464913447697]   \n",
       "3  [0.1629549264907837, 0.14536771178245544, 0.02...   \n",
       "4  [0.2993708550930023, 0.029747437685728073, 0.0...   \n",
       "\n",
       "                                               anger  \\\n",
       "0  [0.08618035167455673, 0.014928092248737812, 0....   \n",
       "1  [0.11253754794597626, 0.08178599923849106, 0.0...   \n",
       "2     [0.05607566609978676, 0, 0.008554211778876683]   \n",
       "3  [0.08566178381443024, 0.06063935160636902, 0.1...   \n",
       "4  [0.024045540019869804, 0.04573961719870567, 0....   \n",
       "\n",
       "                           annoyance  \\\n",
       "0   [None, None, 0.0787773035466671]   \n",
       "1  [None, None, 0.09205344319343567]   \n",
       "2     [None, 0, 0.18107843461136022]   \n",
       "3   [None, None, 0.3909915164113045]   \n",
       "4  [None, None, 0.17476380243897438]   \n",
       "\n",
       "                                             anxiety  \\\n",
       "0  [0.2048976570367813, 0.22192993760108948, 0.01...   \n",
       "1  [0.1962161660194397, 0.03035125695168972, 0.02...   \n",
       "2     [0.17899048328399658, 0, 0.007928416909029087]   \n",
       "3  [0.15674979984760284, 0.08858616650104523, 0.0...   \n",
       "4  [0.07571069151163101, 0.0635836198925972, 0.00...   \n",
       "\n",
       "                                                 awe  \\\n",
       "0  [0.05436728522181511, 0.02884811908006668, 0.0...   \n",
       "1  [0.21560074388980865, 0.030698880553245544, 0....   \n",
       "2     [0.07669304311275482, 0, 0.014362408236290017]   \n",
       "3  [0.2349795550107956, 0.0018904487369582057, 0....   \n",
       "4  [0.06732875853776932, 0.021565068513154984, 0....   \n",
       "\n",
       "                                         awkwardness  \\\n",
       "0  [0.15164369344711304, 0.03940080106258392, 0.0...   \n",
       "1  [0.2403903752565384, 0.036647979170084, 0.0196...   \n",
       "2      [0.17239488661289215, 0, 0.08554266517361005]   \n",
       "3  [0.25021985173225403, 0.6390978097915649, 0.04...   \n",
       "4  [0.14015096426010132, 0.04658352956175804, 0.0...   \n",
       "\n",
       "                                             boredom  ...  \\\n",
       "0  [0.3025617003440857, 0.0033794636838138103, 0....  ...   \n",
       "1  [0.2060079425573349, 0.013258460909128189, 0.0...  ...   \n",
       "2         [0.2141619473695755, 0, 0.257558507223924]  ...   \n",
       "3  [0.2568792998790741, 0.014520961791276932, 0.0...  ...   \n",
       "4  [0.23363099992275238, 0.005424466449767351, 0....  ...   \n",
       "\n",
       "                                             romance  \\\n",
       "0  [0.03606966510415077, 0.003395841922610998, 0....   \n",
       "1  [0.025960002094507217, 0.0028873037081211805, ...   \n",
       "2     [0.10045278072357178, 0, 0.016710910635689896]   \n",
       "3  [0.038216110318899155, 0.005263861268758774, 0...   \n",
       "4  [0.12284506857395172, 0.011654037982225418, 0....   \n",
       "\n",
       "                                             sadness  \\\n",
       "0  [0.5214796662330627, 0.023176860064268112, 0.0...   \n",
       "1  [0.19044746458530426, 0.012958797626197338, 0....   \n",
       "2      [0.2172575294971466, 0, 0.025304595318933327]   \n",
       "3  [0.12238343805074692, 0.05931299924850464, 0.0...   \n",
       "4  [0.12852677702903748, 0.1803823858499527, 0.00...   \n",
       "\n",
       "                              sarcasm  \\\n",
       "0  [None, None, 0.045924624986946584]   \n",
       "1  [None, None, 0.028950467705726624]   \n",
       "2      [None, 0, 0.08060755083958308]   \n",
       "3   [None, None, 0.03817640943452716]   \n",
       "4  [None, None, 0.060818194411695004]   \n",
       "\n",
       "                                        satisfaction  \\\n",
       "0  [0.08109661191701889, 0.00422081770375371, 0.0...   \n",
       "1  [0.06389091908931732, 0.004681485705077648, 0....   \n",
       "2      [0.29512691497802734, 0, 0.05323744503160318]   \n",
       "3  [0.09828534722328186, 0.009946794249117374, 0....   \n",
       "4  [0.5955402851104736, 0.01081307977437973, 0.05...   \n",
       "\n",
       "                                               shame  \\\n",
       "0  [0.13604705035686493, 0.03896016255021095, 0.0...   \n",
       "1  [0.06486066430807114, 0.008367716334760189, 0....   \n",
       "2     [0.11326480656862259, 0, 0.009244329140832027]   \n",
       "3  [0.051911190152168274, 0.2808518707752228, 0.0...   \n",
       "4  [0.05567222088575363, 0.04099632054567337, 0.0...   \n",
       "\n",
       "                                 surprise (negative)  \\\n",
       "0  [0.0438678115606308, 0.12426678091287613, 0.65...   \n",
       "1  [0.23657678067684174, 0.3340807259082794, 0.23...   \n",
       "2    [0.037892572581768036, 0, 0.037050863107045494]   \n",
       "3  [0.17751213908195496, 0.08215691894292831, 0.2...   \n",
       "4  [0.0194998811930418, 0.11610786616802216, 0.01...   \n",
       "\n",
       "                                 surprise (positive)  \\\n",
       "0  [0.021023310720920563, 0.05467357486486435, 0....   \n",
       "1  [0.13827574253082275, 0.06782031059265137, 0.1...   \n",
       "2      [0.032265614718198776, 0, 0.0729232303177317]   \n",
       "3  [0.14006930589675903, 0.024958673864603043, 0....   \n",
       "4  [0.022709805518388748, 0.035930514335632324, 0...   \n",
       "\n",
       "                                            sympathy  \\\n",
       "0  [0.07115799933671951, 0.014430238865315914, 0....   \n",
       "1  [0.05339166149497032, 0.005844631232321262, 0....   \n",
       "2      [0.05577797442674637, 0, 0.01054172085908552]   \n",
       "3  [0.05296207591891289, 0.13199752569198608, 0.0...   \n",
       "4  [0.05819540470838547, 0.05966867133975029, 0.0...   \n",
       "\n",
       "                                           tiredness  \\\n",
       "0  [0.34018707275390625, 0.013311447575688362, 0....   \n",
       "1  [0.1670031100511551, 0.005232616793364286, 0.0...   \n",
       "2      [0.35597968101501465, 0, 0.08751298813149333]   \n",
       "3  [0.15917706489562988, 0.01297526340931654, 0.0...   \n",
       "4  [0.35029497742652893, 0.007299954537302256, 0....   \n",
       "\n",
       "                                             triumph  \n",
       "0  [0.017957456409931183, 0.0021609694231301546, ...  \n",
       "1  [0.029141930863261223, 0.006851397454738617, 0...  \n",
       "2     [0.05453604459762573, 0, 0.011231241204465428]  \n",
       "3  [0.03596600890159607, 0.0024232380092144012, 0...  \n",
       "4  [0.06962665170431137, 0.005042276810854673, 0....  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfused_data = get_emotion_scores(df)\n",
    "unfused_data[all_emotions].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fab4de4-d339-4d41-9a49-0ba49e0bee3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.064811</td>\n",
       "      <td>0.062359</td>\n",
       "      <td>0.171528</td>\n",
       "      <td>0.113535</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.445982</td>\n",
       "      <td>0.163203</td>\n",
       "      <td>0.238960</td>\n",
       "      <td>0.316508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>0.630715</td>\n",
       "      <td>0.045925</td>\n",
       "      <td>0.091712</td>\n",
       "      <td>0.202038</td>\n",
       "      <td>0.818245</td>\n",
       "      <td>0.254803</td>\n",
       "      <td>0.198318</td>\n",
       "      <td>0.358406</td>\n",
       "      <td>0.022734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079566</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.053845</td>\n",
       "      <td>0.155967</td>\n",
       "      <td>0.213493</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.253349</td>\n",
       "      <td>0.260014</td>\n",
       "      <td>0.296702</td>\n",
       "      <td>0.225344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.205501</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.073580</td>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.807173</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.040656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134537</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.084965</td>\n",
       "      <td>0.245321</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.186919</td>\n",
       "      <td>0.091055</td>\n",
       "      <td>0.257938</td>\n",
       "      <td>0.471720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117164</td>\n",
       "      <td>0.242562</td>\n",
       "      <td>0.080608</td>\n",
       "      <td>0.348364</td>\n",
       "      <td>0.122509</td>\n",
       "      <td>0.074943</td>\n",
       "      <td>0.105189</td>\n",
       "      <td>0.066320</td>\n",
       "      <td>0.443493</td>\n",
       "      <td>0.065767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130239</td>\n",
       "      <td>0.094360</td>\n",
       "      <td>0.078352</td>\n",
       "      <td>0.329457</td>\n",
       "      <td>0.306634</td>\n",
       "      <td>0.390992</td>\n",
       "      <td>0.264052</td>\n",
       "      <td>0.254076</td>\n",
       "      <td>0.929878</td>\n",
       "      <td>0.278212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050323</td>\n",
       "      <td>0.187018</td>\n",
       "      <td>0.038176</td>\n",
       "      <td>0.120449</td>\n",
       "      <td>0.339278</td>\n",
       "      <td>0.515166</td>\n",
       "      <td>0.247112</td>\n",
       "      <td>0.190067</td>\n",
       "      <td>0.175593</td>\n",
       "      <td>0.047429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156687</td>\n",
       "      <td>0.200794</td>\n",
       "      <td>0.113619</td>\n",
       "      <td>0.344803</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.174764</td>\n",
       "      <td>0.143977</td>\n",
       "      <td>0.092978</td>\n",
       "      <td>0.214089</td>\n",
       "      <td>0.326228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135891</td>\n",
       "      <td>0.318849</td>\n",
       "      <td>0.060818</td>\n",
       "      <td>0.663136</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.070003</td>\n",
       "      <td>0.126150</td>\n",
       "      <td>0.400166</td>\n",
       "      <td>0.116220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0    0.089333   0.064811                0.062359   0.171528  0.113535   \n",
       "1    0.079566   0.047286                0.053845   0.155967  0.213493   \n",
       "2    0.134537   0.156853                0.084965   0.245321  0.064630   \n",
       "3    0.130239   0.094360                0.078352   0.329457  0.306634   \n",
       "4    0.156687   0.200794                0.113619   0.344803  0.145212   \n",
       "\n",
       "   annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0   0.078777  0.445982  0.163203     0.238960  0.316508  ...  0.042524   \n",
       "1   0.092053  0.253349  0.260014     0.296702  0.225344  ...  0.029184   \n",
       "2   0.181078  0.186919  0.091055     0.257938  0.471720  ...  0.117164   \n",
       "3   0.390992  0.264052  0.254076     0.929878  0.278212  ...  0.050323   \n",
       "4   0.174764  0.143977  0.092978     0.214089  0.326228  ...  0.135891   \n",
       "\n",
       "    sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0  0.630715  0.045925      0.091712  0.202038             0.818245   \n",
       "1  0.205501  0.028950      0.073580  0.075701             0.807173   \n",
       "2  0.242562  0.080608      0.348364  0.122509             0.074943   \n",
       "3  0.187018  0.038176      0.120449  0.339278             0.515166   \n",
       "4  0.318849  0.060818      0.663136  0.108009             0.154099   \n",
       "\n",
       "   surprise (positive)  sympathy  tiredness   triumph  \n",
       "0             0.254803  0.198318   0.358406  0.022734  \n",
       "1             0.332100  0.061174   0.175412  0.040656  \n",
       "2             0.105189  0.066320   0.443493  0.065767  \n",
       "3             0.247112  0.190067   0.175593  0.047429  \n",
       "4             0.070003  0.126150   0.400166  0.116220  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sum_df = get_simple_sum(unfused_data)\n",
    "simple_sum_df[all_emotions].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9edd038-7542-485d-aea8-043799eed4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.019183</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.056042</td>\n",
       "      <td>0.032387</td>\n",
       "      <td>0.032694</td>\n",
       "      <td>0.133353</td>\n",
       "      <td>0.056697</td>\n",
       "      <td>0.072073</td>\n",
       "      <td>0.084032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.178636</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.025080</td>\n",
       "      <td>0.059214</td>\n",
       "      <td>0.321627</td>\n",
       "      <td>0.097579</td>\n",
       "      <td>0.069953</td>\n",
       "      <td>0.094682</td>\n",
       "      <td>0.006450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.014718</td>\n",
       "      <td>0.044376</td>\n",
       "      <td>0.063782</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>0.071929</td>\n",
       "      <td>0.071652</td>\n",
       "      <td>0.082492</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007818</td>\n",
       "      <td>0.054527</td>\n",
       "      <td>0.012015</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.020587</td>\n",
       "      <td>0.268277</td>\n",
       "      <td>0.110258</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>0.046376</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.043624</td>\n",
       "      <td>0.024562</td>\n",
       "      <td>0.067814</td>\n",
       "      <td>0.018108</td>\n",
       "      <td>0.075150</td>\n",
       "      <td>0.049757</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.162488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033013</td>\n",
       "      <td>0.066903</td>\n",
       "      <td>0.033453</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>0.038641</td>\n",
       "      <td>0.018855</td>\n",
       "      <td>0.128733</td>\n",
       "      <td>0.018819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036389</td>\n",
       "      <td>0.027845</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.098375</td>\n",
       "      <td>0.108510</td>\n",
       "      <td>0.162267</td>\n",
       "      <td>0.077285</td>\n",
       "      <td>0.068758</td>\n",
       "      <td>0.289741</td>\n",
       "      <td>0.074239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014474</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>0.015844</td>\n",
       "      <td>0.033822</td>\n",
       "      <td>0.107564</td>\n",
       "      <td>0.178850</td>\n",
       "      <td>0.078550</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>0.013877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042628</td>\n",
       "      <td>0.053416</td>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.093907</td>\n",
       "      <td>0.052428</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.042287</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>0.062894</td>\n",
       "      <td>0.098595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.096184</td>\n",
       "      <td>0.025240</td>\n",
       "      <td>0.181689</td>\n",
       "      <td>0.032499</td>\n",
       "      <td>0.050516</td>\n",
       "      <td>0.022302</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>0.110981</td>\n",
       "      <td>0.036960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0.071324</td>\n",
       "      <td>0.041720</td>\n",
       "      <td>0.035037</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.204077</td>\n",
       "      <td>0.014995</td>\n",
       "      <td>0.084030</td>\n",
       "      <td>0.019564</td>\n",
       "      <td>0.081708</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046908</td>\n",
       "      <td>0.189620</td>\n",
       "      <td>0.016446</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>0.087464</td>\n",
       "      <td>0.030979</td>\n",
       "      <td>0.052773</td>\n",
       "      <td>0.023990</td>\n",
       "      <td>0.088821</td>\n",
       "      <td>0.018995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>0.056486</td>\n",
       "      <td>0.063593</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.158809</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>0.075587</td>\n",
       "      <td>0.055261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028340</td>\n",
       "      <td>0.026341</td>\n",
       "      <td>0.046394</td>\n",
       "      <td>0.108124</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.094530</td>\n",
       "      <td>0.058096</td>\n",
       "      <td>0.023865</td>\n",
       "      <td>0.034967</td>\n",
       "      <td>0.024033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>0.097045</td>\n",
       "      <td>0.117304</td>\n",
       "      <td>0.034304</td>\n",
       "      <td>0.241166</td>\n",
       "      <td>0.032639</td>\n",
       "      <td>0.018955</td>\n",
       "      <td>0.028642</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.104791</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107537</td>\n",
       "      <td>0.027927</td>\n",
       "      <td>0.024383</td>\n",
       "      <td>0.195332</td>\n",
       "      <td>0.018134</td>\n",
       "      <td>0.026767</td>\n",
       "      <td>0.082833</td>\n",
       "      <td>0.030718</td>\n",
       "      <td>0.036446</td>\n",
       "      <td>0.045927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>0.242322</td>\n",
       "      <td>0.118912</td>\n",
       "      <td>0.060184</td>\n",
       "      <td>0.128714</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.078987</td>\n",
       "      <td>0.090496</td>\n",
       "      <td>0.086916</td>\n",
       "      <td>0.077445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044578</td>\n",
       "      <td>0.046066</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.101886</td>\n",
       "      <td>0.019683</td>\n",
       "      <td>0.017684</td>\n",
       "      <td>0.041382</td>\n",
       "      <td>0.046068</td>\n",
       "      <td>0.047189</td>\n",
       "      <td>0.033711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0.065996</td>\n",
       "      <td>0.053870</td>\n",
       "      <td>0.037002</td>\n",
       "      <td>0.209079</td>\n",
       "      <td>0.035115</td>\n",
       "      <td>0.018227</td>\n",
       "      <td>0.040667</td>\n",
       "      <td>0.076994</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>0.036772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030755</td>\n",
       "      <td>0.037378</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>0.088185</td>\n",
       "      <td>0.017005</td>\n",
       "      <td>0.070562</td>\n",
       "      <td>0.084472</td>\n",
       "      <td>0.023820</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.056874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1071 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0       0.027597   0.019183                0.018094   0.056042  0.032387   \n",
       "1       0.021482   0.012613                0.014718   0.044376  0.063782   \n",
       "2       0.037308   0.043624                0.024562   0.067814  0.018108   \n",
       "3       0.036389   0.027845                0.021755   0.098375  0.108510   \n",
       "4       0.042628   0.053416                0.032012   0.093907  0.052428   \n",
       "...          ...        ...                     ...        ...       ...   \n",
       "1066    0.071324   0.041720                0.035037   0.048373  0.204077   \n",
       "1067    0.056486   0.063593                0.026600   0.158809  0.013411   \n",
       "1068    0.097045   0.117304                0.034304   0.241166  0.032639   \n",
       "1069    0.242322   0.118912                0.060184   0.128714  0.017079   \n",
       "1070    0.065996   0.053870                0.037002   0.209079  0.035115   \n",
       "\n",
       "      annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0      0.032694  0.133353  0.056697     0.072073  0.084032  ...  0.011738   \n",
       "1      0.038204  0.071929  0.071652     0.082492  0.060317  ...  0.007818   \n",
       "2      0.075150  0.049757  0.025871     0.080256  0.162488  ...  0.033013   \n",
       "3      0.162267  0.077285  0.068758     0.289741  0.074239  ...  0.014474   \n",
       "4      0.072529  0.042287  0.026191     0.062894  0.098595  ...  0.036261   \n",
       "...         ...       ...       ...          ...       ...  ...       ...   \n",
       "1066   0.014995  0.084030  0.019564     0.081708  0.067054  ...  0.046908   \n",
       "1067   0.046980  0.024601  0.023827     0.075587  0.055261  ...  0.028340   \n",
       "1068   0.018955  0.028642  0.038939     0.104791  0.037836  ...  0.107537   \n",
       "1069   0.006813  0.078987  0.090496     0.086916  0.077445  ...  0.044578   \n",
       "1070   0.018227  0.040667  0.076994     0.082279  0.036772  ...  0.030755   \n",
       "\n",
       "       sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0     0.178636  0.019059      0.025080  0.059214             0.321627   \n",
       "1     0.054527  0.012015      0.020188  0.020587             0.268277   \n",
       "2     0.066903  0.033453      0.098711  0.033241             0.025214   \n",
       "3     0.053279  0.015844      0.033822  0.107564             0.178850   \n",
       "4     0.096184  0.025240      0.181689  0.032499             0.050516   \n",
       "...        ...       ...           ...       ...                  ...   \n",
       "1066  0.189620  0.016446      0.077910  0.087464             0.030979   \n",
       "1067  0.026341  0.046394      0.108124  0.016082             0.094530   \n",
       "1068  0.027927  0.024383      0.195332  0.018134             0.026767   \n",
       "1069  0.046066  0.008377      0.101886  0.019683             0.017684   \n",
       "1070  0.037378  0.017214      0.088185  0.017005             0.070562   \n",
       "\n",
       "      surprise (positive)  sympathy  tiredness   triumph  \n",
       "0                0.097579  0.069953   0.094682  0.006450  \n",
       "1                0.110258  0.016567   0.046376  0.011730  \n",
       "2                0.038641  0.018855   0.128733  0.018819  \n",
       "3                0.078550  0.058818   0.046973  0.013877  \n",
       "4                0.022302  0.037962   0.110981  0.036960  \n",
       "...                   ...       ...        ...       ...  \n",
       "1066             0.052773  0.023990   0.088821  0.018995  \n",
       "1067             0.058096  0.023865   0.034967  0.024033  \n",
       "1068             0.082833  0.030718   0.036446  0.045927  \n",
       "1069             0.041382  0.046068   0.047189  0.033711  \n",
       "1070             0.084472  0.023820   0.025532  0.056874  \n",
       "\n",
       "[1071 rows x 53 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_df = get_relative_weights(unfused_data, 'emotion')\n",
    "weighted_df[all_emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a39839-8115-47f3-a52d-19fb3ff27b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029577</td>\n",
       "      <td>0.021385</td>\n",
       "      <td>0.020496</td>\n",
       "      <td>0.056932</td>\n",
       "      <td>0.037235</td>\n",
       "      <td>0.027463</td>\n",
       "      <td>0.145699</td>\n",
       "      <td>0.054978</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>0.103212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.016010</td>\n",
       "      <td>0.029987</td>\n",
       "      <td>0.066385</td>\n",
       "      <td>0.281427</td>\n",
       "      <td>0.087105</td>\n",
       "      <td>0.067152</td>\n",
       "      <td>0.116717</td>\n",
       "      <td>0.007458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025980</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.051053</td>\n",
       "      <td>0.069961</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.097006</td>\n",
       "      <td>0.073458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.066910</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.024056</td>\n",
       "      <td>0.024690</td>\n",
       "      <td>0.268350</td>\n",
       "      <td>0.111024</td>\n",
       "      <td>0.019950</td>\n",
       "      <td>0.057141</td>\n",
       "      <td>0.013340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044123</td>\n",
       "      <td>0.051461</td>\n",
       "      <td>0.028015</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.063126</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.029956</td>\n",
       "      <td>0.085903</td>\n",
       "      <td>0.159456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038504</td>\n",
       "      <td>0.079497</td>\n",
       "      <td>0.028101</td>\n",
       "      <td>0.114566</td>\n",
       "      <td>0.040069</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>0.021820</td>\n",
       "      <td>0.146311</td>\n",
       "      <td>0.021656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042734</td>\n",
       "      <td>0.031148</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.107780</td>\n",
       "      <td>0.103534</td>\n",
       "      <td>0.136304</td>\n",
       "      <td>0.086403</td>\n",
       "      <td>0.083055</td>\n",
       "      <td>0.303936</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>0.061008</td>\n",
       "      <td>0.013309</td>\n",
       "      <td>0.039475</td>\n",
       "      <td>0.110739</td>\n",
       "      <td>0.173605</td>\n",
       "      <td>0.082320</td>\n",
       "      <td>0.062051</td>\n",
       "      <td>0.057212</td>\n",
       "      <td>0.015642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051162</td>\n",
       "      <td>0.065390</td>\n",
       "      <td>0.037264</td>\n",
       "      <td>0.112556</td>\n",
       "      <td>0.049032</td>\n",
       "      <td>0.060925</td>\n",
       "      <td>0.046995</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>0.070318</td>\n",
       "      <td>0.108160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.104095</td>\n",
       "      <td>0.021202</td>\n",
       "      <td>0.217055</td>\n",
       "      <td>0.035432</td>\n",
       "      <td>0.050650</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.041277</td>\n",
       "      <td>0.131175</td>\n",
       "      <td>0.038780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0.064175</td>\n",
       "      <td>0.039744</td>\n",
       "      <td>0.032310</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>0.208181</td>\n",
       "      <td>0.012596</td>\n",
       "      <td>0.100542</td>\n",
       "      <td>0.020269</td>\n",
       "      <td>0.088862</td>\n",
       "      <td>0.076320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043008</td>\n",
       "      <td>0.228988</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>0.032229</td>\n",
       "      <td>0.046238</td>\n",
       "      <td>0.027791</td>\n",
       "      <td>0.110030</td>\n",
       "      <td>0.018420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>0.069648</td>\n",
       "      <td>0.079387</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.195891</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.039463</td>\n",
       "      <td>0.028765</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.087104</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035394</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>0.038971</td>\n",
       "      <td>0.133481</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.082055</td>\n",
       "      <td>0.052406</td>\n",
       "      <td>0.029083</td>\n",
       "      <td>0.041861</td>\n",
       "      <td>0.028907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>0.108637</td>\n",
       "      <td>0.134545</td>\n",
       "      <td>0.039373</td>\n",
       "      <td>0.278991</td>\n",
       "      <td>0.034102</td>\n",
       "      <td>0.015922</td>\n",
       "      <td>0.031056</td>\n",
       "      <td>0.043380</td>\n",
       "      <td>0.109429</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109774</td>\n",
       "      <td>0.029572</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.223909</td>\n",
       "      <td>0.019679</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.078375</td>\n",
       "      <td>0.032215</td>\n",
       "      <td>0.040728</td>\n",
       "      <td>0.052281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>0.233351</td>\n",
       "      <td>0.128718</td>\n",
       "      <td>0.064713</td>\n",
       "      <td>0.146993</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.084898</td>\n",
       "      <td>0.092022</td>\n",
       "      <td>0.095969</td>\n",
       "      <td>0.092747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.054020</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>0.042839</td>\n",
       "      <td>0.046920</td>\n",
       "      <td>0.057508</td>\n",
       "      <td>0.037345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0.078236</td>\n",
       "      <td>0.064814</td>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.245541</td>\n",
       "      <td>0.041390</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>0.045749</td>\n",
       "      <td>0.092411</td>\n",
       "      <td>0.089059</td>\n",
       "      <td>0.041985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.041455</td>\n",
       "      <td>0.014460</td>\n",
       "      <td>0.105533</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>0.072474</td>\n",
       "      <td>0.092360</td>\n",
       "      <td>0.026433</td>\n",
       "      <td>0.030237</td>\n",
       "      <td>0.069922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1071 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0       0.029577   0.021385                0.020496   0.056932  0.037235   \n",
       "1       0.025980   0.015418                0.017582   0.051053  0.069961   \n",
       "2       0.044123   0.051461                0.028015   0.080424  0.021224   \n",
       "3       0.042734   0.031148                0.025663   0.107780  0.103534   \n",
       "4       0.051162   0.065390                0.037264   0.112556  0.049032   \n",
       "...          ...        ...                     ...        ...       ...   \n",
       "1066    0.064175   0.039744                0.032310   0.053293  0.208181   \n",
       "1067    0.069648   0.079387                0.032593   0.195891  0.014508   \n",
       "1068    0.108637   0.134545                0.039373   0.278991  0.034102   \n",
       "1069    0.233351   0.128718                0.064713   0.146993  0.020592   \n",
       "1070    0.078236   0.064814                0.040973   0.245541  0.041390   \n",
       "\n",
       "      annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0      0.027463  0.145699  0.054978     0.078883  0.103212  ...  0.013907   \n",
       "1      0.032091  0.083064  0.084928     0.097006  0.073458  ...  0.009504   \n",
       "2      0.063126  0.060991  0.029956     0.085903  0.159456  ...  0.038504   \n",
       "3      0.136304  0.086403  0.083055     0.303936  0.090675  ...  0.016534   \n",
       "4      0.060925  0.046995  0.030358     0.070318  0.108160  ...  0.044248   \n",
       "...         ...       ...       ...          ...       ...  ...       ...   \n",
       "1066   0.012596  0.100542  0.020269     0.088862  0.076320  ...  0.043008   \n",
       "1067   0.039463  0.028765  0.027827     0.087104  0.062400  ...  0.035394   \n",
       "1068   0.015922  0.031056  0.043380     0.109429  0.041778  ...  0.109774   \n",
       "1069   0.005723  0.084898  0.092022     0.095969  0.092747  ...  0.050507   \n",
       "1070   0.015310  0.045749  0.092411     0.089059  0.041985  ...  0.033501   \n",
       "\n",
       "       sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0     0.207200  0.016010      0.029987  0.066385             0.281427   \n",
       "1     0.066910  0.010092      0.024056  0.024690             0.268350   \n",
       "2     0.079497  0.028101      0.114566  0.040069             0.025243   \n",
       "3     0.061008  0.013309      0.039475  0.110739             0.173605   \n",
       "4     0.104095  0.021202      0.217055  0.035432             0.050650   \n",
       "...        ...       ...           ...       ...                  ...   \n",
       "1066  0.228988  0.013815      0.075471  0.100507             0.032229   \n",
       "1067  0.032403  0.038971      0.133481  0.019445             0.082055   \n",
       "1068  0.029572  0.020482      0.223909  0.019679             0.025362   \n",
       "1069  0.054020  0.007037      0.116685  0.022668             0.019256   \n",
       "1070  0.041455  0.014460      0.105533  0.018442             0.072474   \n",
       "\n",
       "      surprise (positive)  sympathy  tiredness   triumph  \n",
       "0                0.087105  0.067152   0.116717  0.007458  \n",
       "1                0.111024  0.019950   0.057141  0.013340  \n",
       "2                0.035918  0.021820   0.146311  0.021656  \n",
       "3                0.082320  0.062051   0.057212  0.015642  \n",
       "4                0.023065  0.041277   0.131175  0.038780  \n",
       "...                   ...       ...        ...       ...  \n",
       "1066             0.046238  0.027791   0.110030  0.018420  \n",
       "1067             0.052406  0.029083   0.041861  0.028907  \n",
       "1068             0.078375  0.032215   0.040728  0.052281  \n",
       "1069             0.042839  0.046920   0.057508  0.037345  \n",
       "1070             0.092360  0.026433   0.030237  0.069922  \n",
       "\n",
       "[1071 rows x 53 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_df = get_relative_weights(unfused_data, 'sentiment')\n",
    "weighted_df[all_emotions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713830a0-869a-4113-afc0-8c74e95b6a5c",
   "metadata": {},
   "source": [
    "## **Predicting the Base Emotion**\n",
    "Now we have an intensity score for each complex emotion. How do we combine the complex emotions to predict the base emotion? We try two approaches.\n",
    "1. **Group Average:** The first approach maps the complex emotions into label groups (each complex emotion is assigned one basic emotion and one sentiment -- either 'positive' or 'negative'). We average the intensities of all the complex emotions that correspond to a given label in order to get the average intensity of that label. In other words, to get the average 'anger' intensity, we average the intensities of the complex emotions that correspond to anger: 'anger', 'annoyance', and 'disapproval'.\n",
    "2. **Classifier:** Our second approach is to train a classifier (a small neural network) that learns the relationship between the complex emotions and each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403aa883-6849-41b5-abe4-ba59cd69652e",
   "metadata": {},
   "source": [
    "#### **Predicting by Group Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13d36b06-fe16-49a9-9af6-8c956f107da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_highest_intensity(df, target, show=True):\n",
    "\n",
    "    # get mapping based on prediction target\n",
    "    if target == 'emotion':\n",
    "        mapping = BASIC_TO_COMPLEX\n",
    "    elif target == 'sentiment':\n",
    "        mapping = SENTIMENT_TO_EMOTION\n",
    "    else:\n",
    "        raise Exception('Invalid target')\n",
    "    \n",
    "    labels = sorted(list(mapping.keys()))\n",
    "    intensities = pd.DataFrame()\n",
    "    individual_emotions = pd.DataFrame()\n",
    "\n",
    "    for label in labels:\n",
    "        scores = pd.concat([df[complex_emotion] for complex_emotion in mapping[label]], axis=1)\n",
    "        individual_emotions = pd.concat([individual_emotions, scores], axis=1)\n",
    "\n",
    "        # Suppress runtime warnings for mean of empty slice\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            intensities[label] = np.nanmean(scores, axis=1)\n",
    "        \n",
    "    if show:\n",
    "        print('Intensity of Complex Emotions (Summed Across All Modalities)')\n",
    "        display(individual_emotions.head())\n",
    "        print('Intensity of Each Label (Averaged Across All Complex Emotions Corresponding to that Label)')\n",
    "        display(intensities.head())\n",
    "    return intensities.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e1c4441-d2ab-4502-8ba0-74094fe08dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Complex Emotions (Summed Across All Modalities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>contempt</th>\n",
       "      <th>confusion</th>\n",
       "      <th>craving</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>...</th>\n",
       "      <th>enthusiasm</th>\n",
       "      <th>entrancement</th>\n",
       "      <th>excitement</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>pride</th>\n",
       "      <th>relief</th>\n",
       "      <th>romance</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113535</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.445982</td>\n",
       "      <td>0.238960</td>\n",
       "      <td>0.316508</td>\n",
       "      <td>0.196471</td>\n",
       "      <td>0.496435</td>\n",
       "      <td>0.048553</td>\n",
       "      <td>0.728390</td>\n",
       "      <td>0.118916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.123442</td>\n",
       "      <td>0.081276</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.057112</td>\n",
       "      <td>0.072393</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.088817</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>0.022734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.213493</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.253349</td>\n",
       "      <td>0.296702</td>\n",
       "      <td>0.225344</td>\n",
       "      <td>0.198803</td>\n",
       "      <td>1.931180</td>\n",
       "      <td>0.062355</td>\n",
       "      <td>0.326406</td>\n",
       "      <td>0.019061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020365</td>\n",
       "      <td>0.114934</td>\n",
       "      <td>0.122606</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.063270</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>0.053710</td>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.040656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.186919</td>\n",
       "      <td>0.257938</td>\n",
       "      <td>0.471720</td>\n",
       "      <td>0.159819</td>\n",
       "      <td>0.320666</td>\n",
       "      <td>0.045414</td>\n",
       "      <td>0.421771</td>\n",
       "      <td>0.058894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027105</td>\n",
       "      <td>0.128915</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>0.250635</td>\n",
       "      <td>0.267914</td>\n",
       "      <td>0.103433</td>\n",
       "      <td>0.302503</td>\n",
       "      <td>0.117164</td>\n",
       "      <td>0.065767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.306634</td>\n",
       "      <td>0.390992</td>\n",
       "      <td>0.264052</td>\n",
       "      <td>0.929878</td>\n",
       "      <td>0.278212</td>\n",
       "      <td>0.353361</td>\n",
       "      <td>1.223268</td>\n",
       "      <td>0.078413</td>\n",
       "      <td>0.463078</td>\n",
       "      <td>0.221294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>0.148549</td>\n",
       "      <td>0.181257</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.127224</td>\n",
       "      <td>0.090354</td>\n",
       "      <td>0.069103</td>\n",
       "      <td>0.066740</td>\n",
       "      <td>0.050323</td>\n",
       "      <td>0.047429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.174764</td>\n",
       "      <td>0.143977</td>\n",
       "      <td>0.214089</td>\n",
       "      <td>0.326228</td>\n",
       "      <td>0.250662</td>\n",
       "      <td>0.359962</td>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.249094</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>0.163126</td>\n",
       "      <td>0.180242</td>\n",
       "      <td>0.012297</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>0.394662</td>\n",
       "      <td>0.134558</td>\n",
       "      <td>0.435780</td>\n",
       "      <td>0.135891</td>\n",
       "      <td>0.116220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  annoyance   anxiety  awkwardness   boredom  contempt  confusion  \\\n",
       "0  0.113535   0.078777  0.445982     0.238960  0.316508  0.196471   0.496435   \n",
       "1  0.213493   0.092053  0.253349     0.296702  0.225344  0.198803   1.931180   \n",
       "2  0.064630   0.181078  0.186919     0.257938  0.471720  0.159819   0.320666   \n",
       "3  0.306634   0.390992  0.264052     0.929878  0.278212  0.353361   1.223268   \n",
       "4  0.145212   0.174764  0.143977     0.214089  0.326228  0.250662   0.359962   \n",
       "\n",
       "    craving  disappointment  disapproval  ...  enthusiasm  entrancement  \\\n",
       "0  0.048553        0.728390     0.118916  ...    0.016243      0.123442   \n",
       "1  0.062355        0.326406     0.019061  ...    0.020365      0.114934   \n",
       "2  0.045414        0.421771     0.058894  ...    0.027105      0.128915   \n",
       "3  0.078413        0.463078     0.221294  ...    0.071428      0.148549   \n",
       "4  0.061069        0.249094     0.064394  ...    0.013271      0.163126   \n",
       "\n",
       "   excitement  gratitude       joy      love     pride    relief   romance  \\\n",
       "0    0.081276   0.001927  0.057112  0.072393  0.044806  0.088817  0.042524   \n",
       "1    0.122606   0.001607  0.063270  0.047065  0.049414  0.053710  0.029184   \n",
       "2    0.138100   0.012154  0.250635  0.267914  0.103433  0.302503  0.117164   \n",
       "3    0.181257   0.002884  0.127224  0.090354  0.069103  0.066740  0.050323   \n",
       "4    0.180242   0.012297  0.399806  0.394662  0.134558  0.435780  0.135891   \n",
       "\n",
       "    triumph  \n",
       "0  0.022734  \n",
       "1  0.040656  \n",
       "2  0.065767  \n",
       "3  0.047429  \n",
       "4  0.116220  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Each Label (Averaged Across All Complex Emotions Corresponding to that Label)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.289930</td>\n",
       "      <td>0.071909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266889</td>\n",
       "      <td>0.072645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171692</td>\n",
       "      <td>0.144468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328500</td>\n",
       "      <td>0.110421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179279</td>\n",
       "      <td>0.205024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "0  0.289930  0.071909\n",
       "1  0.266889  0.072645\n",
       "2  0.171692  0.144468\n",
       "3  0.328500  0.110421\n",
       "4  0.179279  0.205024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_simple = pred_highest_intensity(simple_sum_df, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "246c0531-d2d4-4d3f-8a01-1f9180e612c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Complex Emotions (Summed Across All Modalities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>contempt</th>\n",
       "      <th>confusion</th>\n",
       "      <th>craving</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>...</th>\n",
       "      <th>enthusiasm</th>\n",
       "      <th>entrancement</th>\n",
       "      <th>excitement</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>pride</th>\n",
       "      <th>relief</th>\n",
       "      <th>romance</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037235</td>\n",
       "      <td>0.027463</td>\n",
       "      <td>0.145699</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>0.103212</td>\n",
       "      <td>0.064602</td>\n",
       "      <td>0.162604</td>\n",
       "      <td>0.015817</td>\n",
       "      <td>0.241061</td>\n",
       "      <td>0.041455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.027140</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>0.023674</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.029039</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.007458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069961</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.097006</td>\n",
       "      <td>0.073458</td>\n",
       "      <td>0.064992</td>\n",
       "      <td>0.643573</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>0.106612</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.037555</td>\n",
       "      <td>0.040440</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.020659</td>\n",
       "      <td>0.015325</td>\n",
       "      <td>0.016141</td>\n",
       "      <td>0.017534</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.013340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.063126</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.085903</td>\n",
       "      <td>0.159456</td>\n",
       "      <td>0.053172</td>\n",
       "      <td>0.105888</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.139746</td>\n",
       "      <td>0.020531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.042284</td>\n",
       "      <td>0.045489</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.082085</td>\n",
       "      <td>0.087529</td>\n",
       "      <td>0.033940</td>\n",
       "      <td>0.098866</td>\n",
       "      <td>0.038504</td>\n",
       "      <td>0.021656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103534</td>\n",
       "      <td>0.136304</td>\n",
       "      <td>0.086403</td>\n",
       "      <td>0.303936</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>0.116955</td>\n",
       "      <td>0.403154</td>\n",
       "      <td>0.025555</td>\n",
       "      <td>0.152225</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024901</td>\n",
       "      <td>0.048577</td>\n",
       "      <td>0.060417</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.042106</td>\n",
       "      <td>0.029773</td>\n",
       "      <td>0.022672</td>\n",
       "      <td>0.021765</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>0.015642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049032</td>\n",
       "      <td>0.060925</td>\n",
       "      <td>0.046995</td>\n",
       "      <td>0.070318</td>\n",
       "      <td>0.108160</td>\n",
       "      <td>0.084793</td>\n",
       "      <td>0.120542</td>\n",
       "      <td>0.020044</td>\n",
       "      <td>0.082052</td>\n",
       "      <td>0.022448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.053268</td>\n",
       "      <td>0.058746</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.130194</td>\n",
       "      <td>0.128441</td>\n",
       "      <td>0.044165</td>\n",
       "      <td>0.142169</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.038780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  annoyance   anxiety  awkwardness   boredom  contempt  confusion  \\\n",
       "0  0.037235   0.027463  0.145699     0.078883  0.103212  0.064602   0.162604   \n",
       "1  0.069961   0.032091  0.083064     0.097006  0.073458  0.064992   0.643573   \n",
       "2  0.021224   0.063126  0.060991     0.085903  0.159456  0.053172   0.105888   \n",
       "3  0.103534   0.136304  0.086403     0.303936  0.090675  0.116955   0.403154   \n",
       "4  0.049032   0.060925  0.046995     0.070318  0.108160  0.084793   0.120542   \n",
       "\n",
       "    craving  disappointment  disapproval  ...  enthusiasm  entrancement  \\\n",
       "0  0.015817        0.241061     0.041455  ...    0.005663      0.040519   \n",
       "1  0.020330        0.106612     0.006645  ...    0.007099      0.037555   \n",
       "2  0.014823        0.139746     0.020531  ...    0.009449      0.042284   \n",
       "3  0.025555        0.152225     0.077145  ...    0.024901      0.048577   \n",
       "4  0.020044        0.082052     0.022448  ...    0.004626      0.053268   \n",
       "\n",
       "   excitement  gratitude       joy      love     pride    relief   romance  \\\n",
       "0    0.027140   0.000672  0.018807  0.023674  0.014630  0.029039  0.013907   \n",
       "1    0.040440   0.000560  0.020659  0.015325  0.016141  0.017534  0.009504   \n",
       "2    0.045489   0.004237  0.082085  0.087529  0.033940  0.098866  0.038504   \n",
       "3    0.060417   0.001005  0.042106  0.029773  0.022672  0.021765  0.016534   \n",
       "4    0.058746   0.004287  0.130194  0.128441  0.044165  0.142169  0.044248   \n",
       "\n",
       "    triumph  \n",
       "0  0.007458  \n",
       "1  0.013340  \n",
       "2  0.021656  \n",
       "3  0.015642  \n",
       "4  0.038780  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Each Label (Averaged Across All Complex Emotions Corresponding to that Label)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095925</td>\n",
       "      <td>0.023754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.023774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056861</td>\n",
       "      <td>0.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108393</td>\n",
       "      <td>0.036352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059244</td>\n",
       "      <td>0.066984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "0  0.095925  0.023754\n",
       "1  0.087979  0.023774\n",
       "2  0.056861  0.047442\n",
       "3  0.108393  0.036352\n",
       "4  0.059244  0.066984"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_weights = pred_highest_intensity(weighted_df, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ce93506-8b70-4475-b853-664f7332c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(df, target):\n",
    "    pred = pred_highest_intensity(df, target, show=False)\n",
    "    return np.nanmean(df[target] == pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d3d05-b7ff-4e7e-8470-260d5acf4977",
   "metadata": {},
   "source": [
    "#### **Prediction by Group Average: Simple Sum vs. Weighted Sum Accuracy Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fef03912-2ff3-4436-9c39-40fd15849e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting emotion: 32.96\n",
      "Accuracy at predicting sentiment: 44.44\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy at predicting emotion:', round(score(simple_sum_df, 'emotion') * 100, 2))\n",
    "print('Accuracy at predicting sentiment:', round(score(simple_sum_df, 'sentiment') * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b38c770-c647-449f-b87f-e49c37794daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting emotion: 33.43\n",
      "Accuracy at predicting sentiment: 44.54\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy at predicting emotion:', round(score(weighted_df, 'emotion') * 100, 2))\n",
    "print('Accuracy at predicting sentiment:', round(score(weighted_df, 'sentiment') * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01f523-a3c5-4561-bfec-3e18faa7c5c7",
   "metadata": {},
   "source": [
    "**Observations**: Odd that sentiment prediction is identical (a quick look through the data shows that every prediction is the same for all sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3902b-6fdf-4c72-af04-a87da1e18f93",
   "metadata": {},
   "source": [
    "#### **Predicting by Classifier**\n",
    "Let's try training a small neural network that takes in the simple sum complex emotions and predicts the final emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8eda2eb-fa7b-4806-abb6-d20f5074fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbcade85-701a-4c03-8c43-c3be8f20a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.064811</td>\n",
       "      <td>0.062359</td>\n",
       "      <td>0.171528</td>\n",
       "      <td>0.113535</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.445982</td>\n",
       "      <td>0.163203</td>\n",
       "      <td>0.238960</td>\n",
       "      <td>0.316508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>0.630715</td>\n",
       "      <td>0.045925</td>\n",
       "      <td>0.091712</td>\n",
       "      <td>0.202038</td>\n",
       "      <td>0.818245</td>\n",
       "      <td>0.254803</td>\n",
       "      <td>0.198318</td>\n",
       "      <td>0.358406</td>\n",
       "      <td>0.022734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079566</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.053845</td>\n",
       "      <td>0.155967</td>\n",
       "      <td>0.213493</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.253349</td>\n",
       "      <td>0.260014</td>\n",
       "      <td>0.296702</td>\n",
       "      <td>0.225344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.205501</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.073580</td>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.807173</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.040656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134537</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.084965</td>\n",
       "      <td>0.245321</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.186919</td>\n",
       "      <td>0.091055</td>\n",
       "      <td>0.257938</td>\n",
       "      <td>0.471720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117164</td>\n",
       "      <td>0.242562</td>\n",
       "      <td>0.080608</td>\n",
       "      <td>0.348364</td>\n",
       "      <td>0.122509</td>\n",
       "      <td>0.074943</td>\n",
       "      <td>0.105189</td>\n",
       "      <td>0.066320</td>\n",
       "      <td>0.443493</td>\n",
       "      <td>0.065767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130239</td>\n",
       "      <td>0.094360</td>\n",
       "      <td>0.078352</td>\n",
       "      <td>0.329457</td>\n",
       "      <td>0.306634</td>\n",
       "      <td>0.390992</td>\n",
       "      <td>0.264052</td>\n",
       "      <td>0.254076</td>\n",
       "      <td>0.929878</td>\n",
       "      <td>0.278212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050323</td>\n",
       "      <td>0.187018</td>\n",
       "      <td>0.038176</td>\n",
       "      <td>0.120449</td>\n",
       "      <td>0.339278</td>\n",
       "      <td>0.515166</td>\n",
       "      <td>0.247112</td>\n",
       "      <td>0.190067</td>\n",
       "      <td>0.175593</td>\n",
       "      <td>0.047429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156687</td>\n",
       "      <td>0.200794</td>\n",
       "      <td>0.113619</td>\n",
       "      <td>0.344803</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.174764</td>\n",
       "      <td>0.143977</td>\n",
       "      <td>0.092978</td>\n",
       "      <td>0.214089</td>\n",
       "      <td>0.326228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135891</td>\n",
       "      <td>0.318849</td>\n",
       "      <td>0.060818</td>\n",
       "      <td>0.663136</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.070003</td>\n",
       "      <td>0.126150</td>\n",
       "      <td>0.400166</td>\n",
       "      <td>0.116220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0    0.089333   0.064811                0.062359   0.171528  0.113535   \n",
       "1    0.079566   0.047286                0.053845   0.155967  0.213493   \n",
       "2    0.134537   0.156853                0.084965   0.245321  0.064630   \n",
       "3    0.130239   0.094360                0.078352   0.329457  0.306634   \n",
       "4    0.156687   0.200794                0.113619   0.344803  0.145212   \n",
       "\n",
       "   annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0   0.078777  0.445982  0.163203     0.238960  0.316508  ...  0.042524   \n",
       "1   0.092053  0.253349  0.260014     0.296702  0.225344  ...  0.029184   \n",
       "2   0.181078  0.186919  0.091055     0.257938  0.471720  ...  0.117164   \n",
       "3   0.390992  0.264052  0.254076     0.929878  0.278212  ...  0.050323   \n",
       "4   0.174764  0.143977  0.092978     0.214089  0.326228  ...  0.135891   \n",
       "\n",
       "    sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0  0.630715  0.045925      0.091712  0.202038             0.818245   \n",
       "1  0.205501  0.028950      0.073580  0.075701             0.807173   \n",
       "2  0.242562  0.080608      0.348364  0.122509             0.074943   \n",
       "3  0.187018  0.038176      0.120449  0.339278             0.515166   \n",
       "4  0.318849  0.060818      0.663136  0.108009             0.154099   \n",
       "\n",
       "   surprise (positive)  sympathy  tiredness   triumph  \n",
       "0             0.254803  0.198318   0.358406  0.022734  \n",
       "1             0.332100  0.061174   0.175412  0.040656  \n",
       "2             0.105189  0.066320   0.443493  0.065767  \n",
       "3             0.247112  0.190067   0.175593  0.047429  \n",
       "4             0.070003  0.126150   0.400166  0.116220  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = simple_sum_df[all_emotions]\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a19f1806-9726-40c1-8519-b948a526d756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    emotion sentiment\n",
       "0   sadness  negative\n",
       "1  surprise  negative\n",
       "2   neutral   neutral\n",
       "3   sadness  negative\n",
       "4   neutral   neutral"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = simple_sum_df[['emotion', 'sentiment']]\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4002e827-4ed8-4680-afae-c49732838084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, target):\n",
    "    '''\n",
    "    @param target is either 'emotion' or 'sentiment'\n",
    "    '''\n",
    "    inputs = df[all_emotions]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels[target], test_size=0.2, random_state=42)\n",
    "    model = MLPClassifier(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    display_acc = round(accuracy * 100, 2)\n",
    "    print(f\"Accuracy at predicting {target} is {display_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02fe4346-68d9-4568-9f7a-05c52f0c5cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For simple sum:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\J\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting sentiment is 53.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\J\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting emotion is 53.95%\n",
      "For weighted sum:\n",
      "Accuracy at predicting sentiment is 60.0%\n",
      "Accuracy at predicting emotion is 53.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\J\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"For simple sum:\")\n",
    "train_model(simple_sum_df, 'sentiment')\n",
    "train_model(simple_sum_df, 'emotion')\n",
    "\n",
    "print(\"For weighted sum:\")\n",
    "train_model(weighted_df, 'sentiment')\n",
    "train_model(weighted_df, 'emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c760743-d08a-4de5-8201-170000873257",
   "metadata": {},
   "source": [
    "**Observations**: The neural network is much better at predicting emotion but less accurate at predicting sentiment (and even worse than random chance). Changing the random state of the train-test split also significantly changes the accuracy, suggesting that the model is probably overfitting and the small sample size of data is skewing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c75517d-a913-4f56-b439-126c5bdade37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21264\\1547190092.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdisplay_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy at predicting {target} is {display_acc}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munfused_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_emotions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mflattened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# TODO: process nan better\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrain_no_fusion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'emotion'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\J\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "def train_no_fusion(inputs, target):\n",
    "    '''\n",
    "    @param target is either 'emotion' or 'sentiment'\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels[target], test_size=0.2, random_state=42)\n",
    "    model = MLPClassifier(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    display_acc = round(accuracy * 100, 2)\n",
    "    print(f\"Accuracy at predicting {target} is {display_acc}%\")\n",
    "inputs = np.array(unfused_data[all_emotions].map(lambda row: np.array(row)).values.tolist())\n",
    "flattened = inputs.reshape(inputs.shape[0], -1)\n",
    "# TODO: process nan better\n",
    "train_no_fusion(flattened, 'emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efe882-f9fe-46ef-a13e-9e7b87ebcc5e",
   "metadata": {},
   "source": [
    "## **Evaluations**\n",
    "Compare multimodal approaches to single-modality approaches. TODO: generate a heat map that has emotion, sentiment on one hand and lang_only, pros_only, simplexgoup, simplexclassifier, etc. on the other side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b72f3b-36a4-4d59-8948-a26ded63bb8b",
   "metadata": {},
   "source": [
    "## **Selecting Significant Emotions**\n",
    "We graph the intensity across all the emotions, on all modalities. We can then choose a threshold for when an emotion is 'significant,' and only use 'significant' emotions to predict the final sentiment of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48884b6-8c26-45db-bb50-838d53943670",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_intensity = simple_sum_df[all_emotions].mean(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585c696-4ea0-47d3-9b74-bc5482c85190",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = mean_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc4c87-107b-40b8-b778-09c649db321e",
   "metadata": {},
   "source": [
    "# **Note to self: would be good to modularize how modalities are combined (simple vs. relative sum) and then how the basic emotion is predicted (highest intensity vs. neural net)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b4590-8e07-4d6c-84cc-94b41916d38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0frMVtWgEaCE"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
