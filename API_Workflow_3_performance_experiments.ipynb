{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886ecf79",
   "metadata": {
    "id": "886ecf79"
   },
   "source": [
    "# **Notebook 3: Fusion Experiments**\n",
    "In this notebook, we conduct experiments to answer two questions:\n",
    "1. How should we combine the outputs of each input modality?\n",
    "2. How should we predict basic emotions and binary sentiment from complex emotion?\n",
    "\n",
    "Read on for a more detailed explanation of both questions :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VsRSpa3IbBfV",
   "metadata": {
    "id": "VsRSpa3IbBfV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Initialization and Data Processing**\n",
    "We load a JSON of the sentence-by-sentence Hume predictions on the full MELD dataset. See the previous workbooks for how that JSON is generated and cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e95c11",
   "metadata": {
    "id": "99e95c11"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f516e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_df(directory):\n",
    "    '''\n",
    "    converts JSON output to a PD dataframe\n",
    "    each JSON contains the predicted emotions for one sentence\n",
    "    '''\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = json.load(file)\n",
    "                metadata = content['metadata']\n",
    "\n",
    "                # sentiment data\n",
    "                face_emotions = lowercase_keys(content['predicted']['face'])\n",
    "                prosody_emotions = lowercase_keys(content['predicted']['prosody'])\n",
    "                lang_emotions = lowercase_keys(content['predicted']['lang'])\n",
    "\n",
    "                data.append({\n",
    "                    'dialogue_id': metadata['dialogue_id'],\n",
    "                    'time_start': metadata['time_start'],\n",
    "                    'time_end': metadata['time_end'],\n",
    "                    'speaker': metadata['speaker'],\n",
    "                    'emotion': metadata['emotion'],\n",
    "                    'sentiment': metadata['sentiment'],\n",
    "                    'text_content': metadata['text_content'],\n",
    "                    'file_name': metadata['file_name'],\n",
    "                    'face': face_emotions,\n",
    "                    'prosody': prosody_emotions,\n",
    "                    'lang': lang_emotions\n",
    "                })\n",
    "                \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df6b370-4e71-4d2e-9b4a-9b97cc2fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_keys(dictionary):\n",
    "    '''\n",
    "    changes all (String) keys in a dictionary to be fully lower case\n",
    "    '''\n",
    "    return {key.lower(): value for key, value in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020de35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = './dataset/outputs/merged_all'\n",
    "df = load_json_to_df(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec4a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_scores(df):\n",
    "    '''\n",
    "    adds a column for each Hume/complex emotion, which contains a list of form [face_intensity, prosody_intensity, language_intensity]\n",
    "    '''\n",
    "    df_with_emotions = df.copy()\n",
    "    for emotion in all_emotions:\n",
    "        df_with_emotions[emotion] = df.apply(lambda row: [\n",
    "            row['face'].get(emotion, None),\n",
    "            row['prosody'].get(emotion, None),\n",
    "            row['lang'].get(emotion, None)\n",
    "        ], axis=1)\n",
    "            \n",
    "    return df_with_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260b52e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Exploring Our Emotion Data**\n",
    "Hume has three models that predict emotion based on different modality. One model predicts based on language (the words spoken), another based on prosody (tone of voice, pauses, and vocables), and the last on facial expression. The prosody and face models output 48 emotions. The language model outputs 53 emotions. The five additional emotions output by the language model are {'Annoyance', 'Disapproval', 'Enthusiasm', 'Gratitude', 'Sarcasm'}.\n",
    "\n",
    "Critically, the MELD dataset contains many sentences labeled with the 7 basic emotions (anger, sadness, fear, joy, surprise, disgust, and neutral). The Hume outputs are _not_ the same as the MELD dataset labels. Going forward, we will refer to the Hume outputs as \"complex emotions\" and the MELD dataset labels as \"basic emotions.\" As you'll see below, a key part of our work is reducing predictions about \"complex emotions\" to predictions about \"basic emotions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7355fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputted emotions in lang, prosody, and face models: 53 48 48\n",
      "Emotions outputted by language model but not prosody model: {'sarcasm', 'enthusiasm', 'gratitude', 'disapproval', 'annoyance'}\n",
      "Emotions outputted by prosody model but not face model: set()\n",
      "All emotions: ['admiration', 'adoration', 'aesthetic appreciation', 'amusement', 'anger', 'annoyance', 'anxiety', 'awe', 'awkwardness', 'boredom', 'calmness', 'concentration', 'confusion', 'contemplation', 'contempt', 'contentment', 'craving', 'desire', 'determination', 'disappointment', 'disapproval', 'disgust', 'distress', 'doubt', 'ecstasy', 'embarrassment', 'empathic pain', 'enthusiasm', 'entrancement', 'envy', 'excitement', 'fear', 'gratitude', 'guilt', 'horror', 'interest', 'joy', 'love', 'nostalgia', 'pain', 'pride', 'realization', 'relief', 'romance', 'sadness', 'sarcasm', 'satisfaction', 'shame', 'surprise (negative)', 'surprise (positive)', 'sympathy', 'tiredness', 'triumph']\n"
     ]
    }
   ],
   "source": [
    "lang_emotions = set(df['lang'][0].keys())\n",
    "prosody_emotions = set(df['prosody'][0].keys())\n",
    "face_emotions = set(df['face'][0].keys())\n",
    "\n",
    "print(\"Number of outputted emotions in lang, prosody, and face models:\", len(lang_emotions), len(prosody_emotions), len(face_emotions))\n",
    "print(\"Emotions outputted by language model but not prosody model:\", lang_emotions-prosody_emotions)\n",
    "print(\"Emotions outputted by prosody model but not face model:\", prosody_emotions-face_emotions)\n",
    "\n",
    "all_emotions = sorted(list(lang_emotions))\n",
    "print('All emotions:', all_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ef820-b517-478d-8d8b-1bcfb198db18",
   "metadata": {},
   "source": [
    "### **Emotion Maps**\n",
    "\n",
    "Below, we create maps from the output of Hume AI (up to 53 emotions) to the seven basic emotions in our dataset. We also map the Hume AI emotions to positive/negative sentiment. Annotations below are manually created. Interesting future exploration could involve mapping each Hume/complex emotion to a weighted sum of the 7 basic emotions, potentially through training an ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4123edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_TO_COMPLEX = {\n",
    "  'anger': ['anger', 'annoyance', 'disapproval'],\n",
    "  'fear': ['anxiety', 'doubt', 'fear', 'horror'],\n",
    "  'joy': ['admiration', 'adoration', 'amusement', 'contentment', 'desire', 'ecstasy', 'enthusiasm', 'entrancement', 'excitement', 'gratitude', 'joy', 'love', 'pride', 'relief', 'romance', 'triumph'],\n",
    "  'sadness': ['disappointment', 'distress', 'empathic pain', 'guilt', 'nostalgia', 'pain', 'sadness'],\n",
    "  'surprise': ['awe', 'confusion', 'realization', 'surprise (negative)', 'surprise (positive)'],\n",
    "  'disgust': ['contempt', 'disgust', 'envy', 'sarcasm'],\n",
    "  'neutral': ['aesthetic appreciation', 'awkwardness', 'boredom', 'calmness', 'concentration', 'contemplation', 'craving', 'determination', 'embarrassment', 'interest', 'satisfaction', 'shame', 'sympathy', 'tiredness']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99fad0fe-1e8f-4cc6-9f1f-f320a0e10730",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_TO_EMOTION = {\n",
    "  'positive': [\n",
    "    'admiration', 'adoration', 'aesthetic appreciation', 'amusement', 'awe', 'contentment', 'desire', 'ecstasy', 'enthusiasm', 'entrancement', 'excitement', 'gratitude', 'joy', 'love', 'pride', 'relief', 'romance', 'triumph'\n",
    "  ],\n",
    "  'negative': [\n",
    "    'anger', 'annoyance', 'anxiety', 'awkwardness', 'boredom', 'contempt', 'confusion', 'craving', 'disappointment', 'disapproval', 'disgust', 'distress', 'doubt', 'empathic pain', 'embarrassment', 'envy', 'fear', 'guilt', 'horror', 'nostalgia', 'pain', 'sadness', 'sarcasm', 'shame', 'surprise (negative)', 'sympathy', 'tiredness'\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7befd-85b5-44a5-b24e-1f4883e74785",
   "metadata": {},
   "source": [
    "## **Methods To Fuse Modalities**\n",
    "Hume has a facial expression model (predicts based on frame capture of facial expressions in a video), a prosody model (predicts based on signal waveform), and language model (predicts based off words spoken). Each model outputs predictions for up to 53 emotions based on the given input modality. We experiment with two methods to combine the outputs from different modalities to obtain a single number representing the intensity of each emotion.\n",
    "1. The first method is a simple sum. To get the intensity of an emotion, this function sums the intensity predicated for each modality. In other words: `awe_intensity = face_awe_intensity + prosody_awe_intensity + lang_awe_intensity`\n",
    "2. The second method is a relative sum. To get the intensity of an emotion, this function takes sums the intensity predicated for each modality, weighted by the predictive accuracy of that modality alone. In other words: `awe_intensity = face_awe_intensity * relative accuracy of face-only prediction + ... (the same for prosody and language)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42216b4e-170b-40b8-bb38-dafbbface205",
   "metadata": {},
   "source": [
    "### **Method 1: Simple Sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e262ed7a-5680-421d-bc2f-df7d964b2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_sum(df):\n",
    "    '''\n",
    "    to get the intensity of an emotion, this function sums the intensity predicated for each modality. In other words:\n",
    "    awe_intensity = face_awe_intensity + prosody_awe_intensity + lang_awe_intensity\n",
    "    '''\n",
    "    simple_sum_df = df.copy()\n",
    "    for emotion in all_emotions:\n",
    "        simple_sum_df[emotion] = df[emotion].apply(lambda x: sum([i for i in x if i is not None]))\n",
    "    return simple_sum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018bf0c-6432-4b3d-9443-d19a20121bdb",
   "metadata": {},
   "source": [
    "### **Method 2: Relative Sum**\n",
    "We calculate the extent to which each individual modality predicts the final emotion, and use the relative accuracy of each modality to weight the final sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f630fc-1ed3-4207-a16e-7c123b58d0d0",
   "metadata": {},
   "source": [
    "#### **2a: Accuracy of Individual Modalities**\n",
    "First, we test each individual modality to see how predictive it is of the final emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72b17183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df, mapping, modality):\n",
    "    '''\n",
    "    predicts which basic emotion dominates by taking the mean of all complex emotions corresponding to that basic emotion, and choosing the basic emotion with the highest intensity\n",
    "    '''\n",
    "    intensities = pd.DataFrame()\n",
    "    basic_emotions = sorted(list(mapping.keys()))\n",
    "    \n",
    "    for basic_emotion in basic_emotions:\n",
    "        # this is a disgusting list comprehension that came from flattening a long loop\n",
    "        complex_emotions = pd.concat([df[modality].apply(lambda row: row.get(complex_emotion, np.nan)).rename(complex_emotion) for complex_emotion in mapping[basic_emotion]], axis=1)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            intensities[basic_emotion] = np.nanmean(complex_emotions, axis=1) # column names: basic emotions; each row is a sentence; values are the intensities of the basic emotion for that sentence\n",
    "\n",
    "    # drop all rows that have all nan values\n",
    "    intensities = intensities.dropna(how='all')\n",
    "    \n",
    "    y_pred = intensities.idxmax(axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efbd63eb-ea30-4cd4-bc12-5e65c8edc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df, target, modality):\n",
    "    '''\n",
    "    reports the accuracy of the approach above compared to ground truth\n",
    "    @param df with a column `y_pred` and a column with the name `target`\n",
    "    @returns the percent of rows that have identical `y_pred` and `target` values\n",
    "    ''' \n",
    "    if target == 'emotion':\n",
    "        mapping = BASIC_TO_COMPLEX\n",
    "    elif target == 'sentiment':\n",
    "        mapping = SENTIMENT_TO_EMOTION\n",
    "    else:\n",
    "        raise Exception('Invalid target')\n",
    "        \n",
    "    y_pred = get_predictions(df, mapping, modality)\n",
    "    comparable_columns = df[target].loc[y_pred.index]\n",
    "    total_sentences = len(comparable_columns)\n",
    "    accuracy = np.sum(y_pred == comparable_columns) / total_sentences\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff9c7c05-f597-492e-95c5-b56e305523ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_weights(df, show_results=False):\n",
    "    # run on all combinations of emotions and modalities\n",
    "    targets = ['emotion', 'sentiment']\n",
    "    modalities = ['face','prosody','lang']\n",
    "    \n",
    "    results = pd.DataFrame(index=targets, columns=modalities)\n",
    "    \n",
    "    for target in targets:\n",
    "        for modality in modalities:\n",
    "            accuracy = get_accuracy(df, target, modality)\n",
    "            results.at[target, modality] = accuracy\n",
    "    results = results.astype(float)\n",
    "\n",
    "    # calculate and return relative weights\n",
    "    relative_weights = results.apply(lambda row: row / np.sum(row), axis=1)\n",
    "    \n",
    "    if show_results:\n",
    "        print(\"Accuracy by Modality\")\n",
    "        display(results)\n",
    "        print(\"Relative Modality Weights\")\n",
    "        display(relative_weights)\n",
    "    \n",
    "        # plot heatmap\n",
    "        fig, ax = plt.subplots()\n",
    "        heatmap = ax.imshow(results, cmap='viridis', interpolation='nearest')\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(modalities)))\n",
    "        ax.set_yticks(np.arange(len(targets)))\n",
    "        ax.set_xticklabels(modalities)\n",
    "        ax.set_yticklabels(targets)\n",
    "        plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
    "        plt.colorbar(heatmap)\n",
    "        \n",
    "        ax.set_title('Accuracy Scores by Target and Modality')\n",
    "        plt.xlabel('Modalities')\n",
    "        plt.ylabel('Targets')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return relative_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91148db9-e273-4005-8022-d3815e47ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Modality\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face</th>\n",
       "      <th>prosody</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.309524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               face   prosody      lang\n",
       "emotion    0.240964  0.285714  0.309524\n",
       "sentiment  0.481928  0.440476  0.547619"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Modality Weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face</th>\n",
       "      <th>prosody</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>0.288165</td>\n",
       "      <td>0.341681</td>\n",
       "      <td>0.370154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.327837</td>\n",
       "      <td>0.299639</td>\n",
       "      <td>0.372524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               face   prosody      lang\n",
       "emotion    0.288165  0.341681  0.370154\n",
       "sentiment  0.327837  0.299639  0.372524"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unfused_data = get_emotion_scores(df)\n",
    "weights = calc_relative_weights(unfused_data, show_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc58fa1-903a-4873-a8e5-b9b2cf75e789",
   "metadata": {},
   "source": [
    "#### **2b: Fusion**\n",
    "And now let's put those relative weights to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23d02dae-2f97-4fcd-92f0-6ef10a17f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_weights(df, target):\n",
    "    '''\n",
    "    to get the intensity of an emotion, this function takes sums the intensity predicated for each modality, weighted by the predictive accuracy of that modality alone. In other words:\n",
    "    awe_intensity = face_awe_intensity * relative accuracy of face-only prediction + ... (the same for prosody and language)\n",
    "    '''\n",
    "    weights_df = df.copy()\n",
    "    # df with rows of form [face_weight, prosody_weight, lang_weight]\n",
    "    weights = calc_relative_weights(df)\n",
    "    target_weights = list(weights.loc[target])\n",
    "    for emotion in all_emotions:\n",
    "        weights_df[emotion] = df[emotion].apply(lambda row: sum([modality * weight for modality, weight in zip(row, target_weights) if modality is not None]))\n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddfd17-e955-410f-a7d8-8ad568bf68f0",
   "metadata": {},
   "source": [
    "#### **Fused Data**\n",
    "You can see the results below for unfused data, the simple sum, and the relative sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3805800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.14114883542060852, 0.30203521251678467, 0.0...</td>\n",
       "      <td>[0.16079065203666687, 0.1891828179359436, 0.10...</td>\n",
       "      <td>[0.07750532776117325, 0.07918930053710938, 0.0...</td>\n",
       "      <td>[0.27883636951446533, 0.11641153693199158, 0.0...</td>\n",
       "      <td>[0.04992622509598732, 0.0030877224635332823, 0...</td>\n",
       "      <td>[None, None, 0.1343396305321501]</td>\n",
       "      <td>[0.16791842877864838, 0.008799267932772636, 0....</td>\n",
       "      <td>[0.07505017518997192, 0.07745396345853806, 0.0...</td>\n",
       "      <td>[0.20356300473213196, 0.02251557447016239, 0.0...</td>\n",
       "      <td>[0.24328580498695374, 0.005508228205144405, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.10033214092254639, 0.056595005095005035, 0....</td>\n",
       "      <td>[0.29757633805274963, 0.012119029648602009, 0....</td>\n",
       "      <td>[None, None, 0.05288345710589336]</td>\n",
       "      <td>[0.2265051156282425, 0.2577216625213623, 0.085...</td>\n",
       "      <td>[0.11356319487094879, 0.014340449124574661, 0....</td>\n",
       "      <td>[0.041259028017520905, 0.0043494547717273235, ...</td>\n",
       "      <td>[0.036704111844301224, 0.07087815552949905, 0....</td>\n",
       "      <td>[0.08883305639028549, 0.010647913441061974, 0....</td>\n",
       "      <td>[0.20348863303661346, 0.0034130604472011328, 0...</td>\n",
       "      <td>[0.04303706809878349, 0.2292453795671463, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.09453073143959045, 0.04990643635392189, 0.0...</td>\n",
       "      <td>[0.06346559524536133, 0.01741878315806389, 0.0...</td>\n",
       "      <td>[0.06722597777843475, 0.019765634089708328, 0....</td>\n",
       "      <td>[0.10754143446683884, 0.22486062347888947, 0.0...</td>\n",
       "      <td>[0.09366635233163834, 0.006685763597488403, 0....</td>\n",
       "      <td>[None, None, 0.23595255613327026]</td>\n",
       "      <td>[0.18336400389671326, 0.060340285301208496, 0....</td>\n",
       "      <td>[0.12271913141012192, 0.054116811603307724, 0....</td>\n",
       "      <td>[0.21921449899673462, 0.19049564003944397, 0.0...</td>\n",
       "      <td>[0.35429641604423523, 0.010793090797960758, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.04030366241931915, 0.00944355595856905, 0.0...</td>\n",
       "      <td>[0.23218931257724762, 0.022223906591534615, 0....</td>\n",
       "      <td>[None, None, 0.14933811376492181]</td>\n",
       "      <td>[0.08538418263196945, 0.026238925755023956, 0....</td>\n",
       "      <td>[0.07518686354160309, 0.02765893004834652, 0.0...</td>\n",
       "      <td>[0.09615015983581543, 0.12169190496206284, 0.2...</td>\n",
       "      <td>[0.0692388117313385, 0.28519558906555176, 0.02...</td>\n",
       "      <td>[0.0697571262717247, 0.04506014287471771, 0.00...</td>\n",
       "      <td>[0.22366511821746826, 0.004196109250187874, 0....</td>\n",
       "      <td>[0.024739693850278854, 0.024579038843512535, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.058445438742637634, 0.0013833452248945832, ...</td>\n",
       "      <td>[0.046676959842443466, 0.0024462935980409384, ...</td>\n",
       "      <td>[0.03219106048345566, 0.005927511025220156, 0....</td>\n",
       "      <td>[0.1946077197790146, 0.023743847385048866, 0.0...</td>\n",
       "      <td>[0.21686427295207977, 0.05879781395196915, 0.1...</td>\n",
       "      <td>[None, None, 0.21268869414925576]</td>\n",
       "      <td>[0.19364812970161438, 0.40122923254966736, 0.1...</td>\n",
       "      <td>[0.15738582611083984, 0.0032392283901572227, 0...</td>\n",
       "      <td>[0.15038934350013733, 0.1413150131702423, 0.07...</td>\n",
       "      <td>[0.12161242961883545, 0.006110903341323137, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.02294696867465973, 0.0027630277909338474, 0...</td>\n",
       "      <td>[0.3268367350101471, 0.17860350012779236, 0.08...</td>\n",
       "      <td>[None, None, 0.018724664859473706]</td>\n",
       "      <td>[0.10909633338451385, 0.0006503372569568455, 0...</td>\n",
       "      <td>[0.05112047493457794, 0.058977387845516205, 0....</td>\n",
       "      <td>[0.18379783630371094, 0.03887937217950821, 0.0...</td>\n",
       "      <td>[0.10372763127088547, 0.002925654174759984, 0....</td>\n",
       "      <td>[0.04216867685317993, 0.15594087541103363, 0.0...</td>\n",
       "      <td>[0.14748765528202057, 0.010768192820250988, 0....</td>\n",
       "      <td>[0.0958995446562767, 0.0008411008748225868, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.09275802224874496, 0, 0.026196199003607035]</td>\n",
       "      <td>[0.06895953416824341, 0, 0.04240887891501188]</td>\n",
       "      <td>[0.05090191215276718, 0, 0.013817367143929005]</td>\n",
       "      <td>[0.24605320394039154, 0, 0.0740800816565752]</td>\n",
       "      <td>[0.24733345210552216, 0, 0.0015833978832233697]</td>\n",
       "      <td>[None, 0, 0.016212766524404287]</td>\n",
       "      <td>[0.13285985589027405, 0, 0.0023513793130405247]</td>\n",
       "      <td>[0.22540053725242615, 0, 0.010268570622429252]</td>\n",
       "      <td>[0.1643916219472885, 0, 0.11327562108635902]</td>\n",
       "      <td>[0.1840682178735733, 0, 0.04673979105427861]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0356389656662941, 0, 0.008506755577400327]</td>\n",
       "      <td>[0.13398991525173187, 0, 0.00043473238474689424]</td>\n",
       "      <td>[None, 0, 0.012102583423256874]</td>\n",
       "      <td>[0.16115491092205048, 0, 0.045767003670334816]</td>\n",
       "      <td>[0.04881591349840164, 0, 0.0017363607767038047]</td>\n",
       "      <td>[0.1812192052602768, 0, 0.01766480808146298]</td>\n",
       "      <td>[0.16810119152069092, 0, 0.11294473335146904]</td>\n",
       "      <td>[0.03762076795101166, 0, 0.005448827752843499]</td>\n",
       "      <td>[0.1705312877893448, 0, 0.0027313012978993356]</td>\n",
       "      <td>[0.12819571793079376, 0, 0.007175177568569779]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.1433325707912445, 0.007278481964021921, 0.0...</td>\n",
       "      <td>[0.1073203831911087, 0.006279299966990948, 0.0...</td>\n",
       "      <td>[0.0910089910030365, 0.013569509610533714, 0.0...</td>\n",
       "      <td>[0.2124488353729248, 0.0515068881213665, 0.027...</td>\n",
       "      <td>[0.07553469389677048, 0.06707789748907089, 0.0...</td>\n",
       "      <td>[None, None, 0.032969498075544834]</td>\n",
       "      <td>[0.12446752935647964, 0.05787402018904686, 0.0...</td>\n",
       "      <td>[0.14078892767429352, 0.09474913030862808, 0.0...</td>\n",
       "      <td>[0.2186843454837799, 0.02848333865404129, 0.02...</td>\n",
       "      <td>[0.35193687677383423, 0.013702123425900936, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.06752107292413712, 0.005204719956964254, 0....</td>\n",
       "      <td>[0.12867878377437592, 0.017203599214553833, 0....</td>\n",
       "      <td>[None, None, 0.04266087990254164]</td>\n",
       "      <td>[0.1677073985338211, 0.013770622201263905, 0.0...</td>\n",
       "      <td>[0.059703998267650604, 0.006976233329623938, 0...</td>\n",
       "      <td>[0.07389499247074127, 0.26239851117134094, 0.0...</td>\n",
       "      <td>[0.08424931764602661, 0.2062709927558899, 0.07...</td>\n",
       "      <td>[0.06472038477659225, 0.009138940833508968, 0....</td>\n",
       "      <td>[0.20623525977134705, 0.009125432930886745, 0....</td>\n",
       "      <td>[0.04706863686442375, 0.008307026699185371, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          admiration  \\\n",
       "0  [0.14114883542060852, 0.30203521251678467, 0.0...   \n",
       "1  [0.09453073143959045, 0.04990643635392189, 0.0...   \n",
       "2  [0.058445438742637634, 0.0013833452248945832, ...   \n",
       "3     [0.09275802224874496, 0, 0.026196199003607035]   \n",
       "4  [0.1433325707912445, 0.007278481964021921, 0.0...   \n",
       "\n",
       "                                           adoration  \\\n",
       "0  [0.16079065203666687, 0.1891828179359436, 0.10...   \n",
       "1  [0.06346559524536133, 0.01741878315806389, 0.0...   \n",
       "2  [0.046676959842443466, 0.0024462935980409384, ...   \n",
       "3      [0.06895953416824341, 0, 0.04240887891501188]   \n",
       "4  [0.1073203831911087, 0.006279299966990948, 0.0...   \n",
       "\n",
       "                              aesthetic appreciation  \\\n",
       "0  [0.07750532776117325, 0.07918930053710938, 0.0...   \n",
       "1  [0.06722597777843475, 0.019765634089708328, 0....   \n",
       "2  [0.03219106048345566, 0.005927511025220156, 0....   \n",
       "3     [0.05090191215276718, 0, 0.013817367143929005]   \n",
       "4  [0.0910089910030365, 0.013569509610533714, 0.0...   \n",
       "\n",
       "                                           amusement  \\\n",
       "0  [0.27883636951446533, 0.11641153693199158, 0.0...   \n",
       "1  [0.10754143446683884, 0.22486062347888947, 0.0...   \n",
       "2  [0.1946077197790146, 0.023743847385048866, 0.0...   \n",
       "3       [0.24605320394039154, 0, 0.0740800816565752]   \n",
       "4  [0.2124488353729248, 0.0515068881213665, 0.027...   \n",
       "\n",
       "                                               anger  \\\n",
       "0  [0.04992622509598732, 0.0030877224635332823, 0...   \n",
       "1  [0.09366635233163834, 0.006685763597488403, 0....   \n",
       "2  [0.21686427295207977, 0.05879781395196915, 0.1...   \n",
       "3    [0.24733345210552216, 0, 0.0015833978832233697]   \n",
       "4  [0.07553469389677048, 0.06707789748907089, 0.0...   \n",
       "\n",
       "                            annoyance  \\\n",
       "0    [None, None, 0.1343396305321501]   \n",
       "1   [None, None, 0.23595255613327026]   \n",
       "2   [None, None, 0.21268869414925576]   \n",
       "3     [None, 0, 0.016212766524404287]   \n",
       "4  [None, None, 0.032969498075544834]   \n",
       "\n",
       "                                             anxiety  \\\n",
       "0  [0.16791842877864838, 0.008799267932772636, 0....   \n",
       "1  [0.18336400389671326, 0.060340285301208496, 0....   \n",
       "2  [0.19364812970161438, 0.40122923254966736, 0.1...   \n",
       "3    [0.13285985589027405, 0, 0.0023513793130405247]   \n",
       "4  [0.12446752935647964, 0.05787402018904686, 0.0...   \n",
       "\n",
       "                                                 awe  \\\n",
       "0  [0.07505017518997192, 0.07745396345853806, 0.0...   \n",
       "1  [0.12271913141012192, 0.054116811603307724, 0....   \n",
       "2  [0.15738582611083984, 0.0032392283901572227, 0...   \n",
       "3     [0.22540053725242615, 0, 0.010268570622429252]   \n",
       "4  [0.14078892767429352, 0.09474913030862808, 0.0...   \n",
       "\n",
       "                                         awkwardness  \\\n",
       "0  [0.20356300473213196, 0.02251557447016239, 0.0...   \n",
       "1  [0.21921449899673462, 0.19049564003944397, 0.0...   \n",
       "2  [0.15038934350013733, 0.1413150131702423, 0.07...   \n",
       "3       [0.1643916219472885, 0, 0.11327562108635902]   \n",
       "4  [0.2186843454837799, 0.02848333865404129, 0.02...   \n",
       "\n",
       "                                             boredom  ...  \\\n",
       "0  [0.24328580498695374, 0.005508228205144405, 0....  ...   \n",
       "1  [0.35429641604423523, 0.010793090797960758, 0....  ...   \n",
       "2  [0.12161242961883545, 0.006110903341323137, 0....  ...   \n",
       "3       [0.1840682178735733, 0, 0.04673979105427861]  ...   \n",
       "4  [0.35193687677383423, 0.013702123425900936, 0....  ...   \n",
       "\n",
       "                                             romance  \\\n",
       "0  [0.10033214092254639, 0.056595005095005035, 0....   \n",
       "1  [0.04030366241931915, 0.00944355595856905, 0.0...   \n",
       "2  [0.02294696867465973, 0.0027630277909338474, 0...   \n",
       "3      [0.0356389656662941, 0, 0.008506755577400327]   \n",
       "4  [0.06752107292413712, 0.005204719956964254, 0....   \n",
       "\n",
       "                                             sadness  \\\n",
       "0  [0.29757633805274963, 0.012119029648602009, 0....   \n",
       "1  [0.23218931257724762, 0.022223906591534615, 0....   \n",
       "2  [0.3268367350101471, 0.17860350012779236, 0.08...   \n",
       "3   [0.13398991525173187, 0, 0.00043473238474689424]   \n",
       "4  [0.12867878377437592, 0.017203599214553833, 0....   \n",
       "\n",
       "                              sarcasm  \\\n",
       "0   [None, None, 0.05288345710589336]   \n",
       "1   [None, None, 0.14933811376492181]   \n",
       "2  [None, None, 0.018724664859473706]   \n",
       "3     [None, 0, 0.012102583423256874]   \n",
       "4   [None, None, 0.04266087990254164]   \n",
       "\n",
       "                                        satisfaction  \\\n",
       "0  [0.2265051156282425, 0.2577216625213623, 0.085...   \n",
       "1  [0.08538418263196945, 0.026238925755023956, 0....   \n",
       "2  [0.10909633338451385, 0.0006503372569568455, 0...   \n",
       "3     [0.16115491092205048, 0, 0.045767003670334816]   \n",
       "4  [0.1677073985338211, 0.013770622201263905, 0.0...   \n",
       "\n",
       "                                               shame  \\\n",
       "0  [0.11356319487094879, 0.014340449124574661, 0....   \n",
       "1  [0.07518686354160309, 0.02765893004834652, 0.0...   \n",
       "2  [0.05112047493457794, 0.058977387845516205, 0....   \n",
       "3    [0.04881591349840164, 0, 0.0017363607767038047]   \n",
       "4  [0.059703998267650604, 0.006976233329623938, 0...   \n",
       "\n",
       "                                 surprise (negative)  \\\n",
       "0  [0.041259028017520905, 0.0043494547717273235, ...   \n",
       "1  [0.09615015983581543, 0.12169190496206284, 0.2...   \n",
       "2  [0.18379783630371094, 0.03887937217950821, 0.0...   \n",
       "3       [0.1812192052602768, 0, 0.01766480808146298]   \n",
       "4  [0.07389499247074127, 0.26239851117134094, 0.0...   \n",
       "\n",
       "                                 surprise (positive)  \\\n",
       "0  [0.036704111844301224, 0.07087815552949905, 0....   \n",
       "1  [0.0692388117313385, 0.28519558906555176, 0.02...   \n",
       "2  [0.10372763127088547, 0.002925654174759984, 0....   \n",
       "3      [0.16810119152069092, 0, 0.11294473335146904]   \n",
       "4  [0.08424931764602661, 0.2062709927558899, 0.07...   \n",
       "\n",
       "                                            sympathy  \\\n",
       "0  [0.08883305639028549, 0.010647913441061974, 0....   \n",
       "1  [0.0697571262717247, 0.04506014287471771, 0.00...   \n",
       "2  [0.04216867685317993, 0.15594087541103363, 0.0...   \n",
       "3     [0.03762076795101166, 0, 0.005448827752843499]   \n",
       "4  [0.06472038477659225, 0.009138940833508968, 0....   \n",
       "\n",
       "                                           tiredness  \\\n",
       "0  [0.20348863303661346, 0.0034130604472011328, 0...   \n",
       "1  [0.22366511821746826, 0.004196109250187874, 0....   \n",
       "2  [0.14748765528202057, 0.010768192820250988, 0....   \n",
       "3     [0.1705312877893448, 0, 0.0027313012978993356]   \n",
       "4  [0.20623525977134705, 0.009125432930886745, 0....   \n",
       "\n",
       "                                             triumph  \n",
       "0  [0.04303706809878349, 0.2292453795671463, 0.01...  \n",
       "1  [0.024739693850278854, 0.024579038843512535, 0...  \n",
       "2  [0.0958995446562767, 0.0008411008748225868, 0....  \n",
       "3     [0.12819571793079376, 0, 0.007175177568569779]  \n",
       "4  [0.04706863686442375, 0.008307026699185371, 0....  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfused_data = get_emotion_scores(df)\n",
    "unfused_data[all_emotions].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fab4de4-d339-4d41-9a49-0ba49e0bee3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.497286</td>\n",
       "      <td>0.455182</td>\n",
       "      <td>0.171354</td>\n",
       "      <td>0.422780</td>\n",
       "      <td>0.064589</td>\n",
       "      <td>0.134340</td>\n",
       "      <td>0.188499</td>\n",
       "      <td>0.183416</td>\n",
       "      <td>0.324881</td>\n",
       "      <td>0.264678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167547</td>\n",
       "      <td>0.374928</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>0.570166</td>\n",
       "      <td>0.166898</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>0.225609</td>\n",
       "      <td>0.112447</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>0.290697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147180</td>\n",
       "      <td>0.082509</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>0.357095</td>\n",
       "      <td>0.209260</td>\n",
       "      <td>0.235953</td>\n",
       "      <td>0.282273</td>\n",
       "      <td>0.183257</td>\n",
       "      <td>0.462323</td>\n",
       "      <td>0.369657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051142</td>\n",
       "      <td>0.271251</td>\n",
       "      <td>0.149338</td>\n",
       "      <td>0.114887</td>\n",
       "      <td>0.116037</td>\n",
       "      <td>0.510385</td>\n",
       "      <td>0.383444</td>\n",
       "      <td>0.118978</td>\n",
       "      <td>0.232251</td>\n",
       "      <td>0.053298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061011</td>\n",
       "      <td>0.050061</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.226546</td>\n",
       "      <td>0.375812</td>\n",
       "      <td>0.212689</td>\n",
       "      <td>0.725509</td>\n",
       "      <td>0.163190</td>\n",
       "      <td>0.367488</td>\n",
       "      <td>0.156210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.587167</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>0.141794</td>\n",
       "      <td>0.285887</td>\n",
       "      <td>0.110508</td>\n",
       "      <td>0.207715</td>\n",
       "      <td>0.197574</td>\n",
       "      <td>0.100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.111368</td>\n",
       "      <td>0.064719</td>\n",
       "      <td>0.320133</td>\n",
       "      <td>0.248917</td>\n",
       "      <td>0.016213</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>0.235669</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>0.134425</td>\n",
       "      <td>0.012103</td>\n",
       "      <td>0.206922</td>\n",
       "      <td>0.050552</td>\n",
       "      <td>0.198884</td>\n",
       "      <td>0.281046</td>\n",
       "      <td>0.043070</td>\n",
       "      <td>0.173263</td>\n",
       "      <td>0.135371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.155943</td>\n",
       "      <td>0.116508</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.291537</td>\n",
       "      <td>0.147690</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.250694</td>\n",
       "      <td>0.258482</td>\n",
       "      <td>0.269926</td>\n",
       "      <td>0.378194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.147356</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>0.185635</td>\n",
       "      <td>0.068758</td>\n",
       "      <td>0.401719</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.074970</td>\n",
       "      <td>0.218471</td>\n",
       "      <td>0.057874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0    0.497286   0.455182                0.171354   0.422780  0.064589   \n",
       "1    0.147180   0.082509                0.088272   0.357095  0.209260   \n",
       "2    0.061011   0.050061                0.040254   0.226546  0.375812   \n",
       "3    0.118954   0.111368                0.064719   0.320133  0.248917   \n",
       "4    0.155943   0.116508                0.112505   0.291537  0.147690   \n",
       "\n",
       "   annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0   0.134340  0.188499  0.183416     0.324881  0.264678  ...  0.167547   \n",
       "1   0.235953  0.282273  0.183257     0.462323  0.369657  ...  0.051142   \n",
       "2   0.212689  0.725509  0.163190     0.367488  0.156210  ...  0.028038   \n",
       "3   0.016213  0.135211  0.235669     0.277667  0.230808  ...  0.044146   \n",
       "4   0.032969  0.250694  0.258482     0.269926  0.378194  ...  0.073810   \n",
       "\n",
       "    sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0  0.374928  0.052883      0.570166  0.166898             0.190421   \n",
       "1  0.271251  0.149338      0.114887  0.116037             0.510385   \n",
       "2  0.587167  0.018725      0.116293  0.141794             0.285887   \n",
       "3  0.134425  0.012103      0.206922  0.050552             0.198884   \n",
       "4  0.147356  0.042661      0.185635  0.068758             0.401719   \n",
       "\n",
       "   surprise (positive)  sympathy  tiredness   triumph  \n",
       "0             0.225609  0.112447   0.218229  0.290697  \n",
       "1             0.383444  0.118978   0.232251  0.053298  \n",
       "2             0.110508  0.207715   0.197574  0.100607  \n",
       "3             0.281046  0.043070   0.173263  0.135371  \n",
       "4             0.365533  0.074970   0.218471  0.057874  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sum_df = get_simple_sum(unfused_data)\n",
    "simple_sum_df[all_emotions].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9edd038-7542-485d-aea8-043799eed4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.149918</td>\n",
       "      <td>0.054818</td>\n",
       "      <td>0.130318</td>\n",
       "      <td>0.019727</td>\n",
       "      <td>0.049726</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>0.059533</td>\n",
       "      <td>0.102925</td>\n",
       "      <td>0.077868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052181</td>\n",
       "      <td>0.114038</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.185140</td>\n",
       "      <td>0.052059</td>\n",
       "      <td>0.066979</td>\n",
       "      <td>0.078483</td>\n",
       "      <td>0.034036</td>\n",
       "      <td>0.063997</td>\n",
       "      <td>0.097547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045308</td>\n",
       "      <td>0.024841</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.116960</td>\n",
       "      <td>0.069588</td>\n",
       "      <td>0.087339</td>\n",
       "      <td>0.087733</td>\n",
       "      <td>0.056231</td>\n",
       "      <td>0.147734</td>\n",
       "      <td>0.107474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>0.080735</td>\n",
       "      <td>0.055278</td>\n",
       "      <td>0.034778</td>\n",
       "      <td>0.035999</td>\n",
       "      <td>0.177573</td>\n",
       "      <td>0.128136</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.067511</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017752</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.067225</td>\n",
       "      <td>0.119654</td>\n",
       "      <td>0.078728</td>\n",
       "      <td>0.241249</td>\n",
       "      <td>0.047409</td>\n",
       "      <td>0.119673</td>\n",
       "      <td>0.047677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.185460</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.034083</td>\n",
       "      <td>0.046615</td>\n",
       "      <td>0.089646</td>\n",
       "      <td>0.032317</td>\n",
       "      <td>0.068989</td>\n",
       "      <td>0.060734</td>\n",
       "      <td>0.029353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036426</td>\n",
       "      <td>0.035570</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>0.098325</td>\n",
       "      <td>0.071859</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>0.039156</td>\n",
       "      <td>0.068753</td>\n",
       "      <td>0.089301</td>\n",
       "      <td>0.070343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.058760</td>\n",
       "      <td>0.090248</td>\n",
       "      <td>0.012858</td>\n",
       "      <td>0.050152</td>\n",
       "      <td>0.039597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045764</td>\n",
       "      <td>0.034148</td>\n",
       "      <td>0.033796</td>\n",
       "      <td>0.089028</td>\n",
       "      <td>0.046565</td>\n",
       "      <td>0.012204</td>\n",
       "      <td>0.080943</td>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.081174</td>\n",
       "      <td>0.110745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.043504</td>\n",
       "      <td>0.015791</td>\n",
       "      <td>0.054571</td>\n",
       "      <td>0.020357</td>\n",
       "      <td>0.135168</td>\n",
       "      <td>0.122523</td>\n",
       "      <td>0.022184</td>\n",
       "      <td>0.063699</td>\n",
       "      <td>0.017327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.066898</td>\n",
       "      <td>0.063237</td>\n",
       "      <td>0.042520</td>\n",
       "      <td>0.132316</td>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.019543</td>\n",
       "      <td>0.097533</td>\n",
       "      <td>0.064863</td>\n",
       "      <td>0.090952</td>\n",
       "      <td>0.086731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039406</td>\n",
       "      <td>0.039502</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.098215</td>\n",
       "      <td>0.024663</td>\n",
       "      <td>0.068532</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.026419</td>\n",
       "      <td>0.048261</td>\n",
       "      <td>0.032028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.053063</td>\n",
       "      <td>0.053604</td>\n",
       "      <td>0.033394</td>\n",
       "      <td>0.142825</td>\n",
       "      <td>0.031329</td>\n",
       "      <td>0.096814</td>\n",
       "      <td>0.062441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.317645</td>\n",
       "      <td>0.027202</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.056270</td>\n",
       "      <td>0.090316</td>\n",
       "      <td>0.048871</td>\n",
       "      <td>0.030775</td>\n",
       "      <td>0.081763</td>\n",
       "      <td>0.017378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.016965</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.014470</td>\n",
       "      <td>0.042975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.031022</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.057852</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.018267</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>0.011332</td>\n",
       "      <td>0.078601</td>\n",
       "      <td>0.244449</td>\n",
       "      <td>0.006280</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.045496</td>\n",
       "      <td>0.050677</td>\n",
       "      <td>0.040689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.125123</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.106664</td>\n",
       "      <td>0.023012</td>\n",
       "      <td>0.076969</td>\n",
       "      <td>0.071356</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.049238</td>\n",
       "      <td>0.182270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.071861</td>\n",
       "      <td>0.085731</td>\n",
       "      <td>0.036725</td>\n",
       "      <td>0.080506</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>0.034505</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.048629</td>\n",
       "      <td>0.089533</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046947</td>\n",
       "      <td>0.097220</td>\n",
       "      <td>0.033812</td>\n",
       "      <td>0.061193</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>0.034771</td>\n",
       "      <td>0.031227</td>\n",
       "      <td>0.101877</td>\n",
       "      <td>0.018356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0     0.163900   0.149918                0.054818   0.130318  0.019727   \n",
       "1     0.045308   0.024841                0.026600   0.116960  0.069588   \n",
       "2     0.017752   0.014634                0.012092   0.067225  0.119654   \n",
       "3     0.036426   0.035570                0.019783   0.098325  0.071859   \n",
       "4     0.045764   0.034148                0.033796   0.089028  0.046565   \n",
       "..         ...        ...                     ...        ...       ...   \n",
       "79    0.066898   0.063237                0.042520   0.132316  0.038396   \n",
       "80    0.015046   0.013049                0.010729   0.053063  0.053604   \n",
       "81    0.007742   0.009494                0.005128   0.016965  0.002484   \n",
       "82    0.018267   0.014088                0.011332   0.078601  0.244449   \n",
       "83    0.071861   0.085731                0.036725   0.080506  0.047741   \n",
       "\n",
       "    annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0    0.049726  0.055756  0.059533     0.102925  0.077868  ...  0.052181   \n",
       "1    0.087339  0.087733  0.056231     0.147734  0.107474  ...  0.015357   \n",
       "2    0.078728  0.241249  0.047409     0.119673  0.047677  ...  0.008418   \n",
       "3    0.006001  0.039156  0.068753     0.089301  0.070343  ...  0.013419   \n",
       "4    0.012204  0.080943  0.081437     0.081174  0.110745  ...  0.021637   \n",
       "..        ...       ...       ...          ...       ...  ...       ...   \n",
       "79   0.019543  0.097533  0.064863     0.090952  0.086731  ...  0.039406   \n",
       "80   0.033394  0.142825  0.031329     0.096814  0.062441  ...  0.008107   \n",
       "81   0.013348  0.001706  0.003606     0.014470  0.042975  ...  0.008611   \n",
       "82   0.006280  0.106398  0.045496     0.050677  0.040689  ...  0.006132   \n",
       "83   0.034505  0.080200  0.048629     0.089533  0.137405  ...  0.046947   \n",
       "\n",
       "     sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0   0.114038  0.019575      0.185140  0.052059             0.066979   \n",
       "1   0.080735  0.055278      0.034778  0.035999             0.177573   \n",
       "2   0.185460  0.006931      0.034083  0.046615             0.089646   \n",
       "3   0.038772  0.004480      0.063380  0.014710             0.058760   \n",
       "4   0.043504  0.015791      0.054571  0.020357             0.135168   \n",
       "..       ...       ...           ...       ...                  ...   \n",
       "79  0.039502  0.013168      0.098215  0.024663             0.068532   \n",
       "80  0.317645  0.027202      0.030327  0.056270             0.090316   \n",
       "81  0.008716  0.031022      0.010166  0.003438             0.006021   \n",
       "82  0.125123  0.005146      0.106664  0.023012             0.076969   \n",
       "83  0.097220  0.033812      0.061193  0.040314             0.043097   \n",
       "\n",
       "    surprise (positive)  sympathy  tiredness   triumph  \n",
       "0              0.078483  0.034036   0.063997  0.097547  \n",
       "1              0.128136  0.037038   0.067511  0.017000  \n",
       "2              0.032317  0.068989   0.060734  0.029353  \n",
       "3              0.090248  0.012858   0.050152  0.039597  \n",
       "4              0.122523  0.022184   0.063699  0.017327  \n",
       "..                  ...       ...        ...       ...  \n",
       "79             0.070646  0.026419   0.048261  0.032028  \n",
       "80             0.048871  0.030775   0.081763  0.017378  \n",
       "81             0.006246  0.001819   0.057852  0.004199  \n",
       "82             0.071356  0.013900   0.049238  0.182270  \n",
       "83             0.034771  0.031227   0.101877  0.018356  \n",
       "\n",
       "[84 rows x 53 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_df = get_relative_weights(unfused_data, 'emotion')\n",
    "weighted_df[all_emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01a39839-8115-47f3-a52d-19fb3ff27b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156930</td>\n",
       "      <td>0.148592</td>\n",
       "      <td>0.054598</td>\n",
       "      <td>0.136551</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.050045</td>\n",
       "      <td>0.062075</td>\n",
       "      <td>0.059328</td>\n",
       "      <td>0.110288</td>\n",
       "      <td>0.087326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053807</td>\n",
       "      <td>0.125489</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.183495</td>\n",
       "      <td>0.056054</td>\n",
       "      <td>0.068776</td>\n",
       "      <td>0.077239</td>\n",
       "      <td>0.037144</td>\n",
       "      <td>0.071954</td>\n",
       "      <td>0.089660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046966</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.028439</td>\n",
       "      <td>0.111832</td>\n",
       "      <td>0.073281</td>\n",
       "      <td>0.087898</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.058839</td>\n",
       "      <td>0.148546</td>\n",
       "      <td>0.121087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.089052</td>\n",
       "      <td>0.055632</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.037851</td>\n",
       "      <td>0.176965</td>\n",
       "      <td>0.118962</td>\n",
       "      <td>0.037921</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.016958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>0.073967</td>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.079232</td>\n",
       "      <td>0.232372</td>\n",
       "      <td>0.053523</td>\n",
       "      <td>0.119878</td>\n",
       "      <td>0.052312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.046239</td>\n",
       "      <td>0.095453</td>\n",
       "      <td>0.036318</td>\n",
       "      <td>0.064129</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.033132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040168</td>\n",
       "      <td>0.038406</td>\n",
       "      <td>0.021835</td>\n",
       "      <td>0.108262</td>\n",
       "      <td>0.081675</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.044432</td>\n",
       "      <td>0.077720</td>\n",
       "      <td>0.096092</td>\n",
       "      <td>0.077756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014853</td>\n",
       "      <td>0.044089</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.069882</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.065991</td>\n",
       "      <td>0.097184</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>0.056924</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051157</td>\n",
       "      <td>0.038148</td>\n",
       "      <td>0.036855</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.046754</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.083609</td>\n",
       "      <td>0.083094</td>\n",
       "      <td>0.088706</td>\n",
       "      <td>0.124161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024099</td>\n",
       "      <td>0.047890</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.060655</td>\n",
       "      <td>0.022438</td>\n",
       "      <td>0.127223</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>0.024370</td>\n",
       "      <td>0.071504</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.074476</td>\n",
       "      <td>0.071010</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.147239</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.019668</td>\n",
       "      <td>0.094229</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>0.100121</td>\n",
       "      <td>0.094578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>0.013253</td>\n",
       "      <td>0.107043</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.066189</td>\n",
       "      <td>0.068378</td>\n",
       "      <td>0.029021</td>\n",
       "      <td>0.051715</td>\n",
       "      <td>0.032895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.011515</td>\n",
       "      <td>0.052866</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.033608</td>\n",
       "      <td>0.143050</td>\n",
       "      <td>0.031482</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.069105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.344904</td>\n",
       "      <td>0.027377</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.061535</td>\n",
       "      <td>0.085456</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>0.034026</td>\n",
       "      <td>0.089648</td>\n",
       "      <td>0.017582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.017073</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>0.043251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.031221</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.058222</td>\n",
       "      <td>0.004226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.015240</td>\n",
       "      <td>0.012206</td>\n",
       "      <td>0.083060</td>\n",
       "      <td>0.228697</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>0.050465</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>0.045251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.141580</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>0.109278</td>\n",
       "      <td>0.024782</td>\n",
       "      <td>0.082658</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.015613</td>\n",
       "      <td>0.055128</td>\n",
       "      <td>0.180980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.071636</td>\n",
       "      <td>0.086322</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.085637</td>\n",
       "      <td>0.049481</td>\n",
       "      <td>0.034726</td>\n",
       "      <td>0.084943</td>\n",
       "      <td>0.051080</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.150302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047086</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>0.064693</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>0.044436</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>0.033069</td>\n",
       "      <td>0.112198</td>\n",
       "      <td>0.018974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0     0.156930   0.148592                0.054598   0.136551  0.021605   \n",
       "1     0.046966   0.026631                0.028439   0.111832  0.073281   \n",
       "2     0.020015   0.016385                0.013125   0.073967  0.126022   \n",
       "3     0.040168   0.038406                0.021835   0.108262  0.081675   \n",
       "4     0.051157   0.038148                0.036855   0.095357  0.046754   \n",
       "..         ...        ...                     ...        ...       ...   \n",
       "79    0.074476   0.071010                0.046395   0.147239  0.037566   \n",
       "80    0.015925   0.013675                0.011515   0.052866  0.053631   \n",
       "81    0.007792   0.009555                0.005161   0.017073  0.002500   \n",
       "82    0.019850   0.015240                0.012206   0.083060  0.228697   \n",
       "83    0.071636   0.086322                0.038254   0.085637  0.049481   \n",
       "\n",
       "    annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0    0.050045  0.062075  0.059328     0.110288  0.087326  ...  0.053807   \n",
       "1    0.087898  0.092562  0.058839     0.148546  0.121087  ...  0.016562   \n",
       "2    0.079232  0.232372  0.053523     0.119878  0.052312  ...  0.009218   \n",
       "3    0.006040  0.044432  0.077720     0.096092  0.077756  ...  0.014853   \n",
       "4    0.012282  0.083609  0.083094     0.088706  0.124161  ...  0.024099   \n",
       "..        ...       ...       ...          ...       ...  ...       ...   \n",
       "79   0.019668  0.094229  0.064983     0.100121  0.094578  ...  0.044146   \n",
       "80   0.033608  0.143050  0.031482     0.104072  0.069105  ...  0.008433   \n",
       "81   0.013433  0.001717  0.003629     0.014563  0.043251  ...  0.008666   \n",
       "82   0.006320  0.110552  0.050465     0.055215  0.045251  ...  0.006737   \n",
       "83   0.034726  0.084943  0.051080     0.095882  0.150302  ...  0.047086   \n",
       "\n",
       "     sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0   0.125489  0.019700      0.183495  0.056054             0.068776   \n",
       "1   0.089052  0.055632      0.037070  0.037851             0.176965   \n",
       "2   0.191111  0.006975      0.038400  0.046239             0.095453   \n",
       "3   0.044089  0.004509      0.069882  0.016650             0.065991   \n",
       "4   0.047890  0.015892      0.060655  0.022438             0.127223   \n",
       "..       ...       ...           ...       ...                  ...   \n",
       "79  0.042597  0.013253      0.107043  0.026466             0.066189   \n",
       "80  0.344904  0.027377      0.032141  0.061535             0.085456   \n",
       "81  0.008772  0.031221      0.010231  0.003460             0.006059   \n",
       "82  0.141580  0.005179      0.109278  0.024782             0.082658   \n",
       "83  0.106410  0.034028      0.064693  0.043519             0.044436   \n",
       "\n",
       "    surprise (positive)  sympathy  tiredness   triumph  \n",
       "0              0.077239  0.037144   0.071954  0.089660  \n",
       "1              0.118962  0.037921   0.076218  0.016958  \n",
       "2              0.036318  0.064129   0.066225  0.033132  \n",
       "3              0.097184  0.014363   0.056924  0.044700  \n",
       "4              0.117371  0.024370   0.071504  0.018851  \n",
       "..                  ...       ...        ...       ...  \n",
       "79             0.068378  0.029021   0.051715  0.032895  \n",
       "80             0.044908  0.034026   0.089648  0.017582  \n",
       "81             0.006286  0.001830   0.058222  0.004226  \n",
       "82             0.071003  0.015613   0.055128  0.180980  \n",
       "83             0.035687  0.033069   0.112198  0.018974  \n",
       "\n",
       "[84 rows x 53 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_df = get_relative_weights(unfused_data, 'sentiment')\n",
    "weighted_df[all_emotions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713830a0-869a-4113-afc0-8c74e95b6a5c",
   "metadata": {},
   "source": [
    "## **Predicting the Base Emotion**\n",
    "Now we have an intensity score for each complex emotion. How do we combine the complex emotions to predict the base emotion? We try two approaches.\n",
    "1. **Group Average:** The first approach maps the complex emotions into label groups (each complex emotion is assigned one basic emotion and one sentiment -- either 'positive' or 'negative'). We average the intensities of all the complex emotions that correspond to a given label in order to get the average intensity of that label. In other words, to get the average 'anger' intensity, we average the intensities of the complex emotions that correspond to anger: 'anger', 'annoyance', and 'disapproval'.\n",
    "2. **Classifier:** Our second approach is to train a classifier (a small neural network) that learns the relationship between the complex emotions and each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403aa883-6849-41b5-abe4-ba59cd69652e",
   "metadata": {},
   "source": [
    "#### **Predicting by Group Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13d36b06-fe16-49a9-9af6-8c956f107da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_highest_intensity(df, target, show=True):\n",
    "\n",
    "    # get mapping based on prediction target\n",
    "    if target == 'emotion':\n",
    "        mapping = BASIC_TO_COMPLEX\n",
    "    elif target == 'sentiment':\n",
    "        mapping = SENTIMENT_TO_EMOTION\n",
    "    else:\n",
    "        raise Exception('Invalid target')\n",
    "    \n",
    "    labels = sorted(list(mapping.keys()))\n",
    "    intensities = pd.DataFrame()\n",
    "    individual_emotions = pd.DataFrame()\n",
    "\n",
    "    for label in labels:\n",
    "        scores = pd.concat([df[complex_emotion] for complex_emotion in mapping[label]], axis=1)\n",
    "        individual_emotions = pd.concat([individual_emotions, scores], axis=1)\n",
    "\n",
    "        # Suppress runtime warnings for mean of empty slice\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            intensities[label] = np.nanmean(scores, axis=1)\n",
    "        \n",
    "    if show:\n",
    "        print('Intensity of Complex Emotions (Summed Across All Modalities)')\n",
    "        display(individual_emotions.head())\n",
    "        print('Intensity of Each Label (Averaged Across All Complex Emotions Corresponding to that Label)')\n",
    "        display(intensities.head())\n",
    "    return intensities.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e1c4441-d2ab-4502-8ba0-74094fe08dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Complex Emotions (Summed Across All Modalities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>contempt</th>\n",
       "      <th>confusion</th>\n",
       "      <th>craving</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>...</th>\n",
       "      <th>enthusiasm</th>\n",
       "      <th>entrancement</th>\n",
       "      <th>excitement</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>pride</th>\n",
       "      <th>relief</th>\n",
       "      <th>romance</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064589</td>\n",
       "      <td>0.134340</td>\n",
       "      <td>0.188499</td>\n",
       "      <td>0.324881</td>\n",
       "      <td>0.264678</td>\n",
       "      <td>0.158274</td>\n",
       "      <td>0.430952</td>\n",
       "      <td>0.079894</td>\n",
       "      <td>0.471779</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047025</td>\n",
       "      <td>0.140384</td>\n",
       "      <td>0.556677</td>\n",
       "      <td>0.157390</td>\n",
       "      <td>0.631807</td>\n",
       "      <td>0.518840</td>\n",
       "      <td>0.620032</td>\n",
       "      <td>0.151480</td>\n",
       "      <td>0.167547</td>\n",
       "      <td>0.290697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209260</td>\n",
       "      <td>0.235953</td>\n",
       "      <td>0.282273</td>\n",
       "      <td>0.462323</td>\n",
       "      <td>0.369657</td>\n",
       "      <td>0.287472</td>\n",
       "      <td>1.105117</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.417479</td>\n",
       "      <td>0.121117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.159036</td>\n",
       "      <td>0.153965</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.113626</td>\n",
       "      <td>0.080178</td>\n",
       "      <td>0.084449</td>\n",
       "      <td>0.094733</td>\n",
       "      <td>0.051142</td>\n",
       "      <td>0.053298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375812</td>\n",
       "      <td>0.212689</td>\n",
       "      <td>0.725509</td>\n",
       "      <td>0.367488</td>\n",
       "      <td>0.156210</td>\n",
       "      <td>0.342510</td>\n",
       "      <td>0.547943</td>\n",
       "      <td>0.191155</td>\n",
       "      <td>0.812322</td>\n",
       "      <td>0.223473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.070485</td>\n",
       "      <td>0.215362</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.150284</td>\n",
       "      <td>0.070099</td>\n",
       "      <td>0.072736</td>\n",
       "      <td>0.058457</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.248917</td>\n",
       "      <td>0.016213</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>0.159053</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>0.218785</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.103266</td>\n",
       "      <td>0.421755</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.368154</td>\n",
       "      <td>0.146817</td>\n",
       "      <td>0.110861</td>\n",
       "      <td>0.073884</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>0.135371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147690</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.250694</td>\n",
       "      <td>0.269926</td>\n",
       "      <td>0.378194</td>\n",
       "      <td>0.214058</td>\n",
       "      <td>1.681960</td>\n",
       "      <td>0.089899</td>\n",
       "      <td>0.281361</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029783</td>\n",
       "      <td>0.199129</td>\n",
       "      <td>0.205737</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.153202</td>\n",
       "      <td>0.138666</td>\n",
       "      <td>0.096512</td>\n",
       "      <td>0.098886</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.057874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  annoyance   anxiety  awkwardness   boredom  contempt  confusion  \\\n",
       "0  0.064589   0.134340  0.188499     0.324881  0.264678  0.158274   0.430952   \n",
       "1  0.209260   0.235953  0.282273     0.462323  0.369657  0.287472   1.105117   \n",
       "2  0.375812   0.212689  0.725509     0.367488  0.156210  0.342510   0.547943   \n",
       "3  0.248917   0.016213  0.135211     0.277667  0.230808  0.159053   0.257904   \n",
       "4  0.147690   0.032969  0.250694     0.269926  0.378194  0.214058   1.681960   \n",
       "\n",
       "    craving  disappointment  disapproval  ...  enthusiasm  entrancement  \\\n",
       "0  0.079894        0.471779     0.055956  ...    0.047025      0.140384   \n",
       "1  0.065616        0.417479     0.121117  ...    0.007239      0.159036   \n",
       "2  0.191155        0.812322     0.223473  ...    0.003641      0.070485   \n",
       "3  0.067074        0.218785     0.007911  ...    0.249730      0.103266   \n",
       "4  0.089899        0.281361     0.010925  ...    0.029783      0.199129   \n",
       "\n",
       "   excitement  gratitude       joy      love     pride    relief   romance  \\\n",
       "0    0.556677   0.157390  0.631807  0.518840  0.620032  0.151480  0.167547   \n",
       "1    0.153965   0.001228  0.113626  0.080178  0.084449  0.094733  0.051142   \n",
       "2    0.215362   0.001230  0.150284  0.070099  0.072736  0.058457  0.028038   \n",
       "3    0.421755   0.012038  0.368154  0.146817  0.110861  0.073884  0.044146   \n",
       "4    0.205737   0.001223  0.153202  0.138666  0.096512  0.098886  0.073810   \n",
       "\n",
       "    triumph  \n",
       "0  0.290697  \n",
       "1  0.053298  \n",
       "2  0.100607  \n",
       "3  0.135371  \n",
       "4  0.057874  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Each Label (Averaged Across All Complex Emotions Corresponding to that Label)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.194773</td>\n",
       "      <td>0.318450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.262563</td>\n",
       "      <td>0.105710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.357625</td>\n",
       "      <td>0.086515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.132875</td>\n",
       "      <td>0.159165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219410</td>\n",
       "      <td>0.129577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "0  0.194773  0.318450\n",
       "1  0.262563  0.105710\n",
       "2  0.357625  0.086515\n",
       "3  0.132875  0.159165\n",
       "4  0.219410  0.129577"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_simple = pred_highest_intensity(simple_sum_df, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "246c0531-d2d4-4d3f-8a01-1f9180e612c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Complex Emotions (Summed Across All Modalities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>contempt</th>\n",
       "      <th>confusion</th>\n",
       "      <th>craving</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>...</th>\n",
       "      <th>enthusiasm</th>\n",
       "      <th>entrancement</th>\n",
       "      <th>excitement</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>pride</th>\n",
       "      <th>relief</th>\n",
       "      <th>romance</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.050045</td>\n",
       "      <td>0.062075</td>\n",
       "      <td>0.110288</td>\n",
       "      <td>0.087326</td>\n",
       "      <td>0.053072</td>\n",
       "      <td>0.145963</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>0.161921</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>0.046199</td>\n",
       "      <td>0.173402</td>\n",
       "      <td>0.058632</td>\n",
       "      <td>0.204976</td>\n",
       "      <td>0.173374</td>\n",
       "      <td>0.193123</td>\n",
       "      <td>0.049281</td>\n",
       "      <td>0.053807</td>\n",
       "      <td>0.089660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.073281</td>\n",
       "      <td>0.087898</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.148546</td>\n",
       "      <td>0.121087</td>\n",
       "      <td>0.097081</td>\n",
       "      <td>0.376671</td>\n",
       "      <td>0.021513</td>\n",
       "      <td>0.138950</td>\n",
       "      <td>0.045119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.051990</td>\n",
       "      <td>0.048210</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>0.026119</td>\n",
       "      <td>0.027062</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.016958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126022</td>\n",
       "      <td>0.079232</td>\n",
       "      <td>0.232372</td>\n",
       "      <td>0.119878</td>\n",
       "      <td>0.052312</td>\n",
       "      <td>0.113431</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>0.257253</td>\n",
       "      <td>0.083249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>0.070543</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.022971</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>0.033132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081675</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.044432</td>\n",
       "      <td>0.096092</td>\n",
       "      <td>0.077756</td>\n",
       "      <td>0.052799</td>\n",
       "      <td>0.085893</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>0.071806</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093030</td>\n",
       "      <td>0.034215</td>\n",
       "      <td>0.145375</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.129162</td>\n",
       "      <td>0.050411</td>\n",
       "      <td>0.036862</td>\n",
       "      <td>0.024388</td>\n",
       "      <td>0.014853</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046754</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.083609</td>\n",
       "      <td>0.088706</td>\n",
       "      <td>0.124161</td>\n",
       "      <td>0.070047</td>\n",
       "      <td>0.563702</td>\n",
       "      <td>0.029610</td>\n",
       "      <td>0.091361</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011095</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.068222</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.050166</td>\n",
       "      <td>0.045402</td>\n",
       "      <td>0.031474</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>0.024099</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  annoyance   anxiety  awkwardness   boredom  contempt  confusion  \\\n",
       "0  0.021605   0.050045  0.062075     0.110288  0.087326  0.053072   0.145963   \n",
       "1  0.073281   0.087898  0.092562     0.148546  0.121087  0.097081   0.376671   \n",
       "2  0.126022   0.079232  0.232372     0.119878  0.052312  0.113431   0.180594   \n",
       "3  0.081675   0.006040  0.044432     0.096092  0.077756  0.052799   0.085893   \n",
       "4  0.046754   0.012282  0.083609     0.088706  0.124161  0.070047   0.563702   \n",
       "\n",
       "    craving  disappointment  disapproval  ...  enthusiasm  entrancement  \\\n",
       "0  0.025933        0.161921     0.020845  ...    0.017518      0.046199   \n",
       "1  0.021513        0.138950     0.045119  ...    0.002697      0.051990   \n",
       "2  0.058928        0.257253     0.083249  ...    0.001356      0.023330   \n",
       "3  0.022031        0.071806     0.002947  ...    0.093030      0.034215   \n",
       "4  0.029610        0.091361     0.004070  ...    0.011095      0.066263   \n",
       "\n",
       "   excitement  gratitude       joy      love     pride    relief   romance  \\\n",
       "0    0.173402   0.058632  0.204976  0.173374  0.193123  0.049281  0.053807   \n",
       "1    0.048210   0.000457  0.036042  0.026119  0.027062  0.030196  0.016562   \n",
       "2    0.070543   0.000458  0.049300  0.022971  0.024028  0.019358  0.009218   \n",
       "3    0.145375   0.004484  0.129162  0.050411  0.036862  0.024388  0.014853   \n",
       "4    0.068222   0.000456  0.050166  0.045402  0.031474  0.032182  0.024099   \n",
       "\n",
       "    triumph  \n",
       "0  0.089660  \n",
       "1  0.016958  \n",
       "2  0.033132  \n",
       "3  0.044700  \n",
       "4  0.018851  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of Each Label (Averaged Across All Complex Emotions Corresponding to that Label)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065341</td>\n",
       "      <td>0.102673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.088203</td>\n",
       "      <td>0.033789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117147</td>\n",
       "      <td>0.028439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044055</td>\n",
       "      <td>0.054496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072396</td>\n",
       "      <td>0.042476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negative  positive\n",
       "0  0.065341  0.102673\n",
       "1  0.088203  0.033789\n",
       "2  0.117147  0.028439\n",
       "3  0.044055  0.054496\n",
       "4  0.072396  0.042476"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_weights = pred_highest_intensity(weighted_df, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ce93506-8b70-4475-b853-664f7332c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(df, target):\n",
    "    pred = pred_highest_intensity(df, target, show=False)\n",
    "    return np.nanmean(df[target] == pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d3d05-b7ff-4e7e-8470-260d5acf4977",
   "metadata": {},
   "source": [
    "#### **Prediction by Group Average: Simple Sum vs. Weighted Sum Accuracy Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fef03912-2ff3-4436-9c39-40fd15849e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting emotion: 33.33\n",
      "Accuracy at predicting sentiment: 53.57\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy at predicting emotion:', round(score(simple_sum_df, 'emotion') * 100, 2))\n",
    "print('Accuracy at predicting sentiment:', round(score(simple_sum_df, 'sentiment') * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b38c770-c647-449f-b87f-e49c37794daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting emotion: 32.14\n",
      "Accuracy at predicting sentiment: 53.57\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy at predicting emotion:', round(score(weighted_df, 'emotion') * 100, 2))\n",
    "print('Accuracy at predicting sentiment:', round(score(weighted_df, 'sentiment') * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01f523-a3c5-4561-bfec-3e18faa7c5c7",
   "metadata": {},
   "source": [
    "**Observations**: Odd that sentiment prediction is identical (a quick look through the data shows that every prediction is the same for all sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3902b-6fdf-4c72-af04-a87da1e18f93",
   "metadata": {},
   "source": [
    "#### **Predicting by Classifier**\n",
    "Let's try training a small neural network that takes in the simple sum complex emotions and predicts the final emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8eda2eb-fa7b-4806-abb6-d20f5074fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dbcade85-701a-4c03-8c43-c3be8f20a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>adoration</th>\n",
       "      <th>aesthetic appreciation</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>awe</th>\n",
       "      <th>awkwardness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sadness</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>shame</th>\n",
       "      <th>surprise (negative)</th>\n",
       "      <th>surprise (positive)</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>tiredness</th>\n",
       "      <th>triumph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.497286</td>\n",
       "      <td>0.455182</td>\n",
       "      <td>0.171354</td>\n",
       "      <td>0.422780</td>\n",
       "      <td>0.064589</td>\n",
       "      <td>0.134340</td>\n",
       "      <td>0.188499</td>\n",
       "      <td>0.183416</td>\n",
       "      <td>0.324881</td>\n",
       "      <td>0.264678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167547</td>\n",
       "      <td>0.374928</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>0.570166</td>\n",
       "      <td>0.166898</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>0.225609</td>\n",
       "      <td>0.112447</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>0.290697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147180</td>\n",
       "      <td>0.082509</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>0.357095</td>\n",
       "      <td>0.209260</td>\n",
       "      <td>0.235953</td>\n",
       "      <td>0.282273</td>\n",
       "      <td>0.183257</td>\n",
       "      <td>0.462323</td>\n",
       "      <td>0.369657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051142</td>\n",
       "      <td>0.271251</td>\n",
       "      <td>0.149338</td>\n",
       "      <td>0.114887</td>\n",
       "      <td>0.116037</td>\n",
       "      <td>0.510385</td>\n",
       "      <td>0.383444</td>\n",
       "      <td>0.118978</td>\n",
       "      <td>0.232251</td>\n",
       "      <td>0.053298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061011</td>\n",
       "      <td>0.050061</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.226546</td>\n",
       "      <td>0.375812</td>\n",
       "      <td>0.212689</td>\n",
       "      <td>0.725509</td>\n",
       "      <td>0.163190</td>\n",
       "      <td>0.367488</td>\n",
       "      <td>0.156210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.587167</td>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>0.141794</td>\n",
       "      <td>0.285887</td>\n",
       "      <td>0.110508</td>\n",
       "      <td>0.207715</td>\n",
       "      <td>0.197574</td>\n",
       "      <td>0.100607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.111368</td>\n",
       "      <td>0.064719</td>\n",
       "      <td>0.320133</td>\n",
       "      <td>0.248917</td>\n",
       "      <td>0.016213</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>0.235669</td>\n",
       "      <td>0.277667</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>0.134425</td>\n",
       "      <td>0.012103</td>\n",
       "      <td>0.206922</td>\n",
       "      <td>0.050552</td>\n",
       "      <td>0.198884</td>\n",
       "      <td>0.281046</td>\n",
       "      <td>0.043070</td>\n",
       "      <td>0.173263</td>\n",
       "      <td>0.135371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.155943</td>\n",
       "      <td>0.116508</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.291537</td>\n",
       "      <td>0.147690</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.250694</td>\n",
       "      <td>0.258482</td>\n",
       "      <td>0.269926</td>\n",
       "      <td>0.378194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.147356</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>0.185635</td>\n",
       "      <td>0.068758</td>\n",
       "      <td>0.401719</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.074970</td>\n",
       "      <td>0.218471</td>\n",
       "      <td>0.057874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  adoration  aesthetic appreciation  amusement     anger  \\\n",
       "0    0.497286   0.455182                0.171354   0.422780  0.064589   \n",
       "1    0.147180   0.082509                0.088272   0.357095  0.209260   \n",
       "2    0.061011   0.050061                0.040254   0.226546  0.375812   \n",
       "3    0.118954   0.111368                0.064719   0.320133  0.248917   \n",
       "4    0.155943   0.116508                0.112505   0.291537  0.147690   \n",
       "\n",
       "   annoyance   anxiety       awe  awkwardness   boredom  ...   romance  \\\n",
       "0   0.134340  0.188499  0.183416     0.324881  0.264678  ...  0.167547   \n",
       "1   0.235953  0.282273  0.183257     0.462323  0.369657  ...  0.051142   \n",
       "2   0.212689  0.725509  0.163190     0.367488  0.156210  ...  0.028038   \n",
       "3   0.016213  0.135211  0.235669     0.277667  0.230808  ...  0.044146   \n",
       "4   0.032969  0.250694  0.258482     0.269926  0.378194  ...  0.073810   \n",
       "\n",
       "    sadness   sarcasm  satisfaction     shame  surprise (negative)  \\\n",
       "0  0.374928  0.052883      0.570166  0.166898             0.190421   \n",
       "1  0.271251  0.149338      0.114887  0.116037             0.510385   \n",
       "2  0.587167  0.018725      0.116293  0.141794             0.285887   \n",
       "3  0.134425  0.012103      0.206922  0.050552             0.198884   \n",
       "4  0.147356  0.042661      0.185635  0.068758             0.401719   \n",
       "\n",
       "   surprise (positive)  sympathy  tiredness   triumph  \n",
       "0             0.225609  0.112447   0.218229  0.290697  \n",
       "1             0.383444  0.118978   0.232251  0.053298  \n",
       "2             0.110508  0.207715   0.197574  0.100607  \n",
       "3             0.281046  0.043070   0.173263  0.135371  \n",
       "4             0.365533  0.074970   0.218471  0.057874  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = simple_sum_df[all_emotions]\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a19f1806-9726-40c1-8519-b948a526d756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    emotion sentiment\n",
       "0       joy  positive\n",
       "1  surprise  negative\n",
       "2      fear  negative\n",
       "3       joy  positive\n",
       "4  surprise  positive"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = simple_sum_df[['emotion', 'sentiment']]\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4002e827-4ed8-4680-afae-c49732838084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, target):\n",
    "    '''\n",
    "    @param target is either 'emotion' or 'sentiment'\n",
    "    '''\n",
    "    inputs = df[all_emotions]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels[target], test_size=0.2, random_state=42)\n",
    "    model = MLPClassifier(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    display_acc = round(accuracy * 100, 2)\n",
    "    print(f\"Accuracy at predicting {target} is {display_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02fe4346-68d9-4568-9f7a-05c52f0c5cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For simple sum:\n",
      "Accuracy at predicting sentiment is 58.82%\n",
      "Accuracy at predicting emotion is 35.29%\n",
      "For weighted sum:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selenazhang/miniconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at predicting sentiment is 52.94%\n",
      "Accuracy at predicting emotion is 41.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selenazhang/miniconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"For simple sum:\")\n",
    "train_model(simple_sum_df, 'sentiment')\n",
    "train_model(simple_sum_df, 'emotion')\n",
    "\n",
    "print(\"For weighted sum:\")\n",
    "train_model(weighted_df, 'sentiment')\n",
    "train_model(weighted_df, 'emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c760743-d08a-4de5-8201-170000873257",
   "metadata": {},
   "source": [
    "**Observations**: The neural network is much better at predicting emotion but less accurate at predicting sentiment (and even worse than random chance). Changing the random state of the train-test split also significantly changes the accuracy, suggesting that the model is probably overfitting and the small sample size of data is skewing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3c75517d-a913-4f56-b439-126c5bdade37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14114883542060852 0.30203521251678467 0.05410191116747088\n",
      " 0.16079065203666687 0.1891828179359436 0.10520855482900515\n",
      " 0.07750532776117325 0.07918930053710938 0.014659138611302925\n",
      " 0.27883636951446533 0.11641153693199158 0.02753255580766843\n",
      " 0.04992622509598732 0.0030877224635332823 0.011575456037042806 None None\n",
      " 0.1343396305321501 0.16791842877864838 0.008799267932772636\n",
      " 0.01178123894183395 0.07505017518997192 0.07745396345853806\n",
      " 0.030911508858060606 0.20356300473213196 0.02251557447016239\n",
      " 0.09880285165630855 0.24328580498695374 0.005508228205144405\n",
      " 0.015884273121348366 0.3423054814338684 0.009687336161732674\n",
      " 0.05938658619729372 0.3459246754646301 0.02160748839378357\n",
      " 0.0056744201037173085 0.31791257858276367 0.005086667835712433\n",
      " 0.10795268022383635 0.23394042253494263 0.049855489283800125\n",
      " 0.06329205402961144 0.12159013748168945 0.006248381454497576\n",
      " 0.030435216828034475 0.19180044531822205 0.10715804249048233\n",
      " 0.08210356791432087 0.06237035617232323 0.014298676513135433\n",
      " 0.0032249928783410443 0.09351357817649841 0.023951124399900436\n",
      " 0.00206443532414806 0.09416238218545914 0.07545696943998337\n",
      " 0.006848517321766569 0.2746553122997284 0.021328555420041084\n",
      " 0.17579536999647433 None None 0.05595635111407878 0.07960755378007889\n",
      " 0.003056790679693222 0.005824978955878088 0.23977652192115784\n",
      " 0.01969945803284645 0.018306648421387833 0.2783169150352478\n",
      " 0.011320492252707481 0.05616667441343172 0.05812076851725578\n",
      " 0.1542762815952301 0.007219967135684923 0.14877603948116302\n",
      " 0.023529386147856712 0.03819774091243744 0.11194640398025513\n",
      " 0.0050990162417292595 0.010688720109800879 None None 0.047024912702349514\n",
      " 0.11473175883293152 0.013317147269845009 0.012335543771489309\n",
      " 0.04021960869431496 0.02479529194533825 0.006406750431499229\n",
      " 0.13025063276290894 0.3862661123275757 0.04015992768108845\n",
      " 0.10001793503761292 0.0027248847763985395 0.003291398275625677 None None\n",
      " 0.15739049529656768 0.11396358907222748 0.03302116319537163\n",
      " 0.02546055752855654 0.04222097992897034 0.0016457841265946627\n",
      " 0.0014708600434600017 0.3391791880130768 0.0798974558711052\n",
      " 0.03382714135715595 0.23450101912021637 0.2731470465660095\n",
      " 0.12415862067316014 0.254650741815567 0.11699797213077545\n",
      " 0.14719176776886272 0.09368368983268738 0.18486401438713074\n",
      " 0.031214554597122166 0.20078569650650024 0.002990775741636753\n",
      " 0.010142537057757951 0.07989649474620819 0.47037580609321594\n",
      " 0.06975934366122462 0.16043545305728912 0.2807595431804657\n",
      " 0.17390208691358566 0.09821292012929916 0.037867382168769836\n",
      " 0.015399670138811836 0.10033214092254639 0.056595005095005035\n",
      " 0.010619768796739383 0.29757633805274963 0.012119029648602009\n",
      " 0.06523270433983551 None None 0.05288345710589336 0.2265051156282425\n",
      " 0.2577216625213623 0.08593898288045938 0.11356319487094879\n",
      " 0.014340449124574661 0.03899438170572886 0.041259028017520905\n",
      " 0.0043494547717273235 0.14481255164943063 0.036704111844301224\n",
      " 0.07087815552949905 0.11802701515933642 0.08883305639028549\n",
      " 0.010647913441061974 0.012966428990834035 0.20348863303661346\n",
      " 0.0034130604472011328 0.011327637511735352 0.04303706809878349\n",
      " 0.2292453795671463 0.01841489501440754]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nMLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m flattened \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mreshape(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# TODO: process nan better\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_no_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 7\u001b[0m, in \u001b[0;36mtrain_no_fusion\u001b[0;34m(inputs, target)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(inputs, labels[target], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLPClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m      9\u001b[0m display_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:753\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    737\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:442\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_layer_sizes must be > 0, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m hidden_layer_sizes\n\u001b[1;32m    437\u001b[0m     )\n\u001b[1;32m    438\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefs_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m incremental\n\u001b[1;32m    440\u001b[0m )\n\u001b[0;32m--> 442\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Ensure y is 2D\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1093\u001b[0m, in \u001b[0;36mMLPClassifier._validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, incremental, reset):\n\u001b[0;32m-> 1093\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1102\u001b[0m         y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nMLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "def train_no_fusion(inputs, target):\n",
    "    '''\n",
    "    @param target is either 'emotion' or 'sentiment'\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels[target], test_size=0.2, random_state=42)\n",
    "    model = MLPClassifier(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    display_acc = round(accuracy * 100, 2)\n",
    "    print(f\"Accuracy at predicting {target} is {display_acc}%\")\n",
    "inputs = np.array(unfused_data[all_emotions].map(lambda row: np.array(row)).values.tolist())\n",
    "flattened = inputs.reshape(inputs.shape[0], -1)\n",
    "# TODO: process nan better\n",
    "train_no_fusion(flattened, 'emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efe882-f9fe-46ef-a13e-9e7b87ebcc5e",
   "metadata": {},
   "source": [
    "## **Evaluations**\n",
    "Compare multimodal approaches to single-modality approaches. TODO: generate a heat map that has emotion, sentiment on one hand and lang_only, pros_only, simplexgoup, simplexclassifier, etc. on the other side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b72f3b-36a4-4d59-8948-a26ded63bb8b",
   "metadata": {},
   "source": [
    "## **Selecting Significant Emotions**\n",
    "We graph the intensity across all the emotions, on all modalities. We can then choose a threshold for when an emotion is 'significant,' and only use 'significant' emotions to predict the final sentiment of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48884b6-8c26-45db-bb50-838d53943670",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_intensity = simple_sum_df[all_emotions].mean(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585c696-4ea0-47d3-9b74-bc5482c85190",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = mean_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc4c87-107b-40b8-b778-09c649db321e",
   "metadata": {},
   "source": [
    "# **Note to self: would be good to modularize how modalities are combined (simple vs. relative sum) and then how the basic emotion is predicted (highest intensity vs. neural net)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b4590-8e07-4d6c-84cc-94b41916d38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0frMVtWgEaCE"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
